%&latex
% Created from /l/mt/JOSOverview.tex

\newcommand{\theTitle}{Sequence Models from a Signal Processing Perspective}
\newcommand{\theEvent}{\htmladdnormallink{West Coast Machine Learning}{https://www.youtube.com/channel/UCuoNQWLuEYwjP7mI23sZ3WQ}}
\newcommand{\theDate}{June 13, 2024}

%% \newcommand{\theTitle}{Inventing Modern Sequence Models as a Music 320 Project}
%% \newcommand{\theSubTitle}{Samples become Meaning Vectors}
%% \newcommand{\theEvent}{CCRMA Open House}
%% \newcommand{\theDate}{May 17, 2024}
%% % Classroom (Knoll 217), Friday 1:40-1:55pm}

\newcommand{\theAuthor}{\htmladdnormallink{Julius Smith}{http://ccrma.stanford.edu/~jos/}}

%Mohonk05 said:
\input ../latex/stdpreshdr.tex % /w/latex/stdpreshdr.tex
\input ../latex/wgtmac.tex % /w/latex/wgtmac.tex

\usepackage{xcolor}
%N:\usepackage[dvipsnames]{xcolor}
%N:\usepackage[svgnames]{xcolor}

% Uses package etoolbox for \newtoggle et al:
\usepackage{etoolbox} % for \newtoggle et al
\newtoggle{local}
%\toggletrue{local}
\togglefalse{local}
\input localremote.tex

\date{\theDate}

\title{\theTitle}
\author{\theAuthor\\
%  CCRMA Open House\\
%  Classroom (Knoll 217)\\
%  Stanford University \\[10pt]
  \theEvent
}

\begin{document}

\maketitle

\section[\sectopts]{Background}

\begin{slide}[\slideopts,toc={Path to CCRMA}]{My Path to
    \htmladdnormallink{\textbf{CCRMA}}{http://ccrma.stanford.edu/}
    (Center for Computer Research in Music and Acoustics)
}

\centerline{Musician : Math : Physics : EE : Control : DSP : System ID : SAIL/CCRMA}
\vspace{1em}

\myTwoFiguresToBoxes{Julius}{jtm45_new2}{0.4\twidth}{0.6\textheight}{Some Gig}{Tube Amp}{}
%\myTwoFiguresToBoxes{Bittersweet2}{jtm45_new2}{0.4\twidth}{0.6\textheight}{Some Gig}{Tube Amp}{}
%\myTwoFiguresToBoxes{Julius}{Bittersweet2}{0.4\twidth}{0.6\textheight}{High School Band}{Tube Amp}{}
%\myTwoFiguresToBoxes{Julius}{moogschemsmall}{0.4\twidth}{0.6\textheight}{Some Gig}{Moog VCF Ladder}{}
%\myTwoFiguresToWidth{Julius}{wdelt}{0.4\twidth}{}{}{}
%\myTwoFiguresToWidth{Julius}{conicalcap2}{0.5\twidth}{Tesseract Gig}{Cone Proof}{}
%\myFigureRotateToWidth{Julius}{-90}{0.9\twidth}{}
%\centerline{JOS in situ}
\end{slide}

\input courses-overview.tex

%\end{document}
%\endinput

%\input abstract.tex

\section[\sectopts,toc={Basic Idea}]{Music 320 Project Idea}

\begin{slide}[\slideopts,toc={One Pole Filter}]{Assignment: Do Something Cool with a \emph{One-Pole Recursive Digital Filter}}
\vspace{-1em}
\myFigureToWidth{one-pole-jos}{0.5\twidth}{
  One Pole at $z = p$\\[5pt]
    $y(n) = g\, x(n) + p\, y(n-1)$, $n=0,1,2,\ldots$\\[5pt]
    $H(z) = \frac{\displaystyle Y(z)}{\displaystyle X(z)} = \frac{\displaystyle  g}{\displaystyle 1 - p\,\zi}$}
\maybepause
\vspace{-1em}
\textbf{Idea: Let's Make an Associative Memory!}
\begin{itemize}
\mpitem $x(n)$ can be a \emph{long vector} $\xv(n)\in\RN$ representing \emph{anything we want} --- any ``label''
\mpitem Set $\geev = 1$ and $\peev = 1$ to make $\yv(n)$ a \emph{sum of all input vectors} (``integrator'')
\mpitem Choose the dimension $N$ so large that \emph{vectors in the sum are mostly orthogonal}
\mpitem Retrieve similar vectors using a \emph{matched inner product} $\wv^T\xv > b$,\\
        for some suitable threshold $b$ (Hey! That's a simulated neuron! (``Perceptron''))
\end{itemize}
\end{slide}

\begin{slide}[\slideopts,toc={Inner Product}]{Vector Retrieval by Inner Product}

  Given the sum of vectors
  \[
  \yv(n) = \sum_{m=0}^n \xv(m)
  \]
  and a ``query vector'' $\wv = \xv(k)$,\\
  \maybepause
  find the query in the sum using an \emph{inner product:}
  \[
  \wv^T\yv(n) \eqsp \sum_{m=0}^n \wv^T\xv(m) \;\approx\; \xv^T(k)\,\xv(k) \eqsp \|\xv(k)\|^2 \;>\; b(k)
  \]
  where $b(k)$ is the \emph{detection threshold} for $\xv(k)$

  \begin{itemize}
  \mpitem This works because the spatial dimension is so large that $\xv^T(j)\,\xv(k)\approx \epsilon$ for $j\ne k$

  \mpitem Retrieval threshold $b(k)$ depends on $\|\xv(k)\|^2$\\
  $\Rightarrow$ \textbf{suggestion:} \emph{reserve the radial dimension for similarity scoring}

  \mpitem \Ie, \emph{only populate the \textbf{hypersphere}} in $\RN$: $\norm{\xv(k)} = 1, \, \forall k$

  \mpitem We just invented \textbf{\texttt{RMSNorm}}, used extensively in neural networks (not \texttt{LayerNorm})

  \end{itemize}

\end{slide}

%---------------------------------------------------------------------------------------------------

\begin{slide}[\slideopts,toc={Orthogonality}]{Orthogonality in High Dimensions}
\vspace{-1em}
% \input orthogonality.tex
Let $\ab\in\reals^N$ and $\bb\in\reals^N$ be two normally random, real, unit-norm vectors in $N$ dimensions.
$\|\ab\|=\|\bb\|=1$.

The dot-product of
$\ab^T=[a_1,a_2,\ldots,a_N]$ and
$\bb^T=[b_1,b_2,\ldots,b_N]$ is defined as
\[
\ab \cdot \bb = \sum_{i=1}^{N} a_i b_i.
\]

The squared dot product is
\[
(\ab \cdot \bb)^2 = \left(\sum_{i=1}^{N} a_i b_i\right)^2 = \sum_{i=1}^{N} \sum_{j=1}^{N} a_i a_j b_i b_j.
\]

Expected value (average):
\[
E\left[(\ab \cdot \bb)^2\right]
= \sum_{i=1}^{N} \sum_{j=1}^{N} E[a_i a_j] \, E[b_i b_j]
= \sum_{i=1}^{N} \frac{1}{N}\,\frac{1}{N}
= \zbox{\frac{1}{N}}
\]
\end{slide}

\begin{slide}[\slideopts,toc={}]{Orthogonality in High Dimensions, Continued}
\vspace{-1em}
We just showed the \emph{expected squared dot product of two normally random unit vectors in $\reals^N$ is $1/N$}, \ie,
\[
E\left[(\ab \cdot \bb)^2\right] = \zbox{\frac{1}{N}}
\]
since $E[a_i b_j]=0$ for $i \ne j$, $E[a_i^2] = E[b_i]^2 = 1/N$, and $\ab$ and $\bb$ are independent.\\
\maybepause
\textbf{Suggestions:}
\begin{itemize}
\mpitem \emph{Initialize biases larger than $1/N$}
\mpitem \emph{Divide the sum of $M$ vectors by $\sqrt{M}$:}
\begin{itemize}
  \item ``power normalization''
  \mpitem ``\tx{RMSNorm}-preserving''
  \mpitem Done in \htmladdnormallink{Hawk \& Griffin}{https://arxiv.org/abs/2402.19427}, \eg
  \mpitem ``Keep vector sums near the unit sphere''
\end{itemize}
\mpitem Apply \tx{RMSNorm} when \emph{training} the initial \emph{vocabulary embedding} (``\tx{word2sphere}'')
\mpitem Set the \emph{model dimension} just sufficient for the \emph{layer width} at each level
\end{itemize}
% Note to self: embeddings have semantic proximity while feature maps may not

\end{slide}

\begin{slide}[\slideopts,toc={Model Dimension}]{Model Dimension vs.~Model \& Vocab Size
(\htmladdnormallink{ChatGPT-4o}{https://chatgpt.com/share/76499bbc-95f7-48bf-8646-19786c1961f4}, not checked)}

\vspace{-2em}

% \usepackage{multicol}
% \usepackage{amsmath}

% \input ModelDims.tex

\begin{multicols}{2}
\begin{itemize}
    \item \textbf{Original Transformer}
    \begin{itemize}
      \item $d = 512$ ($65$ M)
    \end{itemize}

    \item \textbf{BERT}
    \begin{itemize}
        \item $d = 768$ ($110$ M, $30$ K)
        \item $d = 1024$ ($340$ M, $30$ K)
    \end{itemize}

    \item \textbf{GPT-2}
    \begin{itemize}
        \item $d = 768$ ($124$ M, $50$ K)
        \item $d = 1024$ ($345$ M, $50$ K)
        \item $d = 1280$ ($774$ M, $50$ K)
        \item $d = 1600$ ($1.5$ B, $50$ K)
    \end{itemize}

    \item \textbf{GPT-3}
    \begin{itemize}
        \item $d = 2048$ ($2.7$ B, $50$ K)
        \item $d = 4096$ ($6.7$ B, $50$ K)
        \item $d = 6144$ ($13$ B, $50$ K)
        \item $d = 12288$ ($175$ B, $50$ K)
    \end{itemize}

    \item \textbf{T5}
    \begin{itemize}
        \item $d = 512$ ($60$ M, $32$ K)
        \item $d = 768$ ($220$ M, $32$ K)
        \item $d = 1024$ ($770$ M, $32$ K)
        \item $d = 1024$ ($3$ B, $32$ K)
        \item $d = 1024$ ($11$ B, $32$ K)
    \end{itemize}

    %% \item \textbf{RoBERTa}
    %% \begin{itemize}
    %%     \item $d = 768$ ($125$ M, $50$ K)
    %%     \item $d = 1024$ ($355$ M, $50$ K)
    %% \end{itemize}

    \item \textbf{ALBERT}
    \begin{itemize}
        \item $d = 768$ ($12$ M, $30$ K)
        \item $d = 1024$ ($18$ M, $30$ K)
        \item $d = 2048$ ($60$ M, $30$ K)
        \item $d = 4096$ ($235$ M, $30$ K)
    \end{itemize}

    \item \textbf{DistilBERT}
    \begin{itemize}
        \item $d = 768$ ($66$ M, $30$ K)
    \end{itemize}

    \item \textbf{Megatron-Turing NLG}
    \begin{itemize}
        \item $d = 20480$ ($530$ B, $50$ K)
    \end{itemize}

\end{itemize}
\end{multicols}

\end{slide}

\begin{slidewhite}[\slideopts,toc={}]{Log10 Model Dimension versus Log10 Model Size}

\vspace{-2em}

%\myFigureToWidth{Model_Dimension_vs_Log10_Parameters}{\twidth}{}
\myFigureToWidth{Model_Dimensions_vs_Model_Sizes_Labeled}{\twidth}{}

\end{slidewhite}

%---------------------------------------------------------------------------------------------------

\section[\sectopts,toc={Architectures}]{Architectures}

\begin{slide}[\slideopts,toc={Vector Memory}]{Cumulative Vector Memory}
\vspace{-1em}
\myFigureToWidth{integrator}{0.6\twidth}{}
\vspace{-1em}
\myFigureToWidth{ginaSphere1}{0.5\twidth}{}
%\myTwoFiguresToWidth{integrator}{ginaSphere1}{0.5\twidth}{Vector summer}{MidJourney depiction for $N=3$}{}
\end{slide}

\begin{slide}[\slideopts,toc={Gating}]{Gated Vector Memory}

\myFigureToWidth{integrator}{0.6\twidth}{Input Vector Summer}

\begin{itemize}
\item \textbf{Problem:} Need a \emph{memory reset}
\mpitem \textbf{Solution:} Set \emph{feedback gain to zero} for one step to clear the memory
\item[]
\mpitem \textbf{Problem:} Need an \emph{input gate} to suppress unimportant inputs
\mpitem \textbf{Solution:} Set \emph{input gain to zero} for unimportant inputs
\item[]
\mpitem We just invented \textbf{gating}, used extensively in neural sequence models
\end{itemize}

\end{slide}

\begin{slide}[\slideopts,toc={Gated RNN}]{Gated Recurrent Network}

\vspace{-1em}

  \textbf{Idea:} \emph{Learn} the input and feedback gates as functions of input $\xv_n$\\
  based on many input-output examples $(\xv_n,\yv_n)$ (``training data''):
%% \mpitem Some item
%% \end{itemize}

\vspace{-1em}

\myFigureToWidth{one-pole-rnn}{0.4\twidth}{Vector Memory with Learned Input and Feedback Gates}

\vspace{-1em}

\maybepause

\textbf{Suggestions:}
\begin{itemize}
\mpitem Use learned, input-based, \emph{activations} for gating (LSTM, GRU, Mamba smoothed input)
\mpitem While activated, \emph{optionally} set \emph{memory duration} via $\peev$ magnitude (SSMs, Mamba)
\begin{itemize}
  \mpitem \emph{Initialize} $\peev$ for desired initial memory duration (exponential fade time)
  \mpitem Learn $\peev(\xv_n)$ as $\Imtx\cdot e^{-\Delta}\approx \Imtx -\Imtx \Delta$,
  %and $\geev(\xv_n)$ as $\mbox{Linear}(\xv_n,\yv_n)\cdot\Delta$,
  where $\Delta = \mbox{softPlus}(\mbox{parameter}(\xv_n,\Yv_n))$ (guaranteed stable --- no ``exploding gradients'')
  [Also multiply $\geev(\xv_n)$ by $\Delta$] % for gain-normalization]
%\mpitem \emph{memory duration} $\Rightarrow$ \emph{linear projection}\\
  % (SSMs, Mamba) ---
%  \mpitem SSMs led to \emph{linear projection} (\emph{no activation} in feedback)
%  based on cool \emph{history-polynomial-approximation} ideas that ultimately went away (S4D), but \emph{linearity survives}.\\
  \mpitem Consider \emph{separate meaning-driven activation} multiplying feedback: $\sigma(\Lmtx\xv)\peev(\xv)$
%  Example: activation by end-of-sentence detector, end-of-paragraph, etc. (e.g., for RAG sentence/paragraph embeddings)\\
%  Analogy: clear Transformer input context buffer after processing a full sentence/paragraph
\end{itemize}
\end{itemize}

\end{slide}

\begin{slide}[\slideopts,toc={Skip Connection}]{Output Gating}

  \vspace{-1em}

  \textbf{Idea:} Since we have input and feedback gates, why not an \textbf{\emph{output gate} and bypass?}

\myFigureToWidth{one-pole-rnn-skip}{0.6\twidth}{Gated RNN with \textbf{Skip Connection}}

\maybepause
Output gating allows network to be ``bypassed'' when not helpful.

\begin{itemize}
\mpitem \textbf{``Obvious'' Suggestion:} The bypass path should be scaled for \emph{power normalization}\\
\mpitem \textbf{Better yet:} Don't scale the bypass and use \tx{RMSNorm} at the input of the next layer\\
(prevents a ``bad layer'' from isolating deeper layers from the input with garbage)
\end{itemize}

\end{slide}

\begin{slide}[\slideopts]{State Expansion}

\vspace{-1em}

  \textbf{Idea:} \emph{Expand} vector-memory dimension to an integer multiple of the model dimension:
% \vspace{-4em}
% \myFigureToWidth{one-pole-rnn-seq}{0.6\twidth}{} % {Sequence Modeling with Gated RNNs}
%

\myFigureToWidth{statespace-rnn}{0.5\twidth}{}

\vspace{-1em}

\maybepause

\emph{``Structured State-Space Models''} (SSM) look like this (\eg, Mamba)
\begin{itemize}
  \mpitem Increased storage capacity
  \mpitem Feedback matrix $\Amtx$ typically \emph{diagonal} since 2022 (see ``S4D'')\\
  	  \maybepause $\Rightarrow$ Parallel bank of vector one-poles (\emph{``linearly'' gated, state-expanded RNNs})
  \mpitem In Mamba-2, $\Amtx = p\,\Imtx$, \ie, \emph{shared memory duration} across expanded state
  % \mpitem Processed sequence (``context buffer'') is \emph{indefinitely long}
  % \mpitem Multiple parallel memories (as in ``multi-head attention'')
  \mpitem Gating matrices in Mamba[-2] are simple linear input projections:
%  \[
  $
          [\Bmtx(\xv_n), \Cmtx(\xv_n)] = \Lmtx\,\xv_n  % , \qquad \Dmtx(\xv_n) = \mbox{SiLU}(\Lmtx^\prime\xv_n)\]
  $
%  \]
  % (See Mamba, \eg)
  % \mpitem Conv1D mixing followed by SiLU on input for Mamba (only nonlinear activation)
  % \mpitem $\Cmtx$ could become an \emph{attention matrix} across the expanded state; $\Amtx$ could make it \emph{shift} like a
  %    \emph{transformer context buffer} (using a unit subdiagonal, \eg)
  % \mpitem $\Amtx = -p\Imtx$ as of Mamba-2, \ie, parallel RNNs all have the same memory duration
\end{itemize}

\end{slide}

\section[\sectopts,toc={Processing}]{Memory Access}

\begin{slide}[\slideopts,toc={Perceptrons}]{Detecting Multiple Vectors in Parallel}
\maybepause
\vspace{1em}
\textbf{Idea:} Detect multiple memory vectors in parallel using an array of \emph{``Perceptrons''}
%\emph{``Multi-Layer Perceptron'' (MLP)}
\begin{itemize}
  \mpitem Each Perceptron detects one or more memory vectors similar to its weight vectors
  \[
  y_i(n) = \wv_i^T\hv(n) > b_i
  \]
  where $y_i(n)$ denotes the $i$th output at time $n$, i=0,\ldots,M-1,\\
  and $\hv(n)$ denotes the (``hidden'' [expanded-] state) vector memory 
  \mpitem Note that the $\Cmtx$ matrix can provide these weights:
  \[
  \yv(n) = \Cmtx\,\hv(n) > \beev
  \]

\mpitem The Perceptrons indicate \emph{which weight-vectors $\wv_i$ are present} in the vector memory $\hv$

\mpitem \textbf{Idea:} (Backpropagation---1980s?): To facilitate
\emph{learning} $\wv_i$ via gradient descent, replace ``$>$'' by
something smoother, such as $1+\tanh[\Cmtx\,\hv(n) - \beev]$

\end{itemize}

\end{slide}

\begin{slide}[\slideopts,toc={Sequences}]{Sequence Modeling}
  \vspace{-1em}
  \begin{itemize}
    \mpitem If each vector represents a \emph{word,} a vector sum is simply a \emph{bag of words}
    \mpitem To model a \emph{sequence} of words, we have various \emph{sequence-position-encoding} options:
    \begin{enumerate}
      \mpitem \emph{Amplitude Decay} - Multiply the sum by a \emph{forgetting factor} each sequence step\\
      (RNNs) - \emph{poor choice} (conflates with angular distance on the hypersphere)
      \mpitem \emph{Sinusoidal Amplitude Modulation} - Add a sinusoid with \emph{increasing frequency} to each vector summing into the history\\
      (used in the original Transformer)
      \mpitem \emph{Phase Shift} - Multiply by the sum by $e^{j\Delta}$ each sample\\
      (``RoPE'') - \emph{apparently most used today}
    \end{enumerate}
    % Toward Xformer: \mpitem Use many vector-sum memories in parallel, positionally encoded (``State Expansion'' in SSMs)
    % \mpitem Learn position-specific \emph{FIR Coefficients} across a parallel bank of positionally encoded vector-sum memories (``Attention Layer'' in a Transformer)
  \end{itemize}
\end{slide}

\begin{slide}[\slideopts,toc={WRoPE}]{RoPE and WRoPE}

\vspace{-1em}

\begin{itemize}
\item Rotational Positional Encoding (RoPE) owns \emph{one arc direction} along the hypersphere % to \emph{positional encoding}
  \mpitem We can thus rotate our vector memory $\hv(n)$ by $\Delta$ radians each time step to ``age'' it:
  \[
  \hv_a(n) = e^{j\Delta}\hv(n), \quad \mbox{with}\; \Delta = \frac{2\pi}{L}
  \]
  when our maximum sequence length (before reset) is $L$
  \mpitem \textbf{Idea:} ``Warped RoPE'' (WRoPE) for \emph{arbitrarily long sequences}:
  \[
  \Delta_n = \frac{2\pi n}{n+1}, \quad n=0,1,2,\ldots
  \]
  \maybepause
  (inspired by the \emph{bilinear transform} used in digital filter design)
  \mpitem A \emph{blend} of \emph{uniform} and \emph{warped} rotations can be used:
  \[
  \Delta_n = \funcalign{\frac{\pi n}{L}}{n=0,1,2,\ldots,L-1}{%
    \pi + \frac{\pi n}{n+1}}{n=L,L+1,L+2,\ldots}
  \]
  where $L$ is now the \emph{typical} sequence length (giving it more ``space'' in recall)
\end{itemize}
\end{slide}

\section[\sectopts,toc={Attention}]{Attention}

\begin{slide}[\slideopts,toc={Attention}]{Attention Layer}

\vspace{-1em}

\textbf{Idea:} Also use \emph{FIR Filtering} (SSM State Expansion Factor $N\ge M$):

%\myFigureToWidth{one-pole-rnn-seq}{0.6\twidth}{} % {Sequence Modeling with Gated RNNs}
%\vspace{-4em}

\myFigureToWidth{attention}{0.8\twidth}{}

\vspace{-1em}

\emph{Separately learnable FIR coefficient matrices} $\dv_k[\xv(n)], \cv_j[\xv(n-j),k]$, depending on
  \begin{enumerate}
    \mpitem input \emph{position} $j$ in the input sequence (``context buffer'' or ``expanded state'' + RoPE),
    \mpitem input \emph{vector} $\xv(n-j)$, $j=0,1,2,\dots,M$,
    \mpitem \emph{output-position} $k$ being computed, $k=0,1,2,\dots,M$ ($M+1$ outputs)
  \end{enumerate}

\maybepause
\textbf{Idea:} Add \emph{relevance gating} suppressing unimportant inputs to each output (``attention'')

\vspace{0.2em}

\maybepause
\textbf{Idea:} Create \emph{new embedding vectors} as \emph{sums} of existing embedding vectors (``attention'')

\vspace{0.2em}

\maybepause
\textbf{Idea:} Measure relevance using an \emph{inner product} between the output and input positions (``dot-product attention'')
\end{slide}

\begin{slide}[\slideopts]{Dot-Product Attention}

\vspace{-2em}

\myFigureToWidth{attention}{0.8\twidth}{}

\vspace{-2em}

\textbf{Relevance Gating}

Let $\xv_k$ denote $\xv(n-k)$

The contribution from input $\xv_j$ to the nonlinear FIR sum for output $\yv_k$ can be calculated as
\[
\cv_{kj} = [(\sum_{m\in \Rscr_k}\xv_m)^T \xv_j]\xv_j
\]

\maybepause
or more generally $\cv_{kj} = (Q_k^T \xv_j)\xv_j$, where
\\
%\begin{itemize}
%  \mpitem
$Q_k[\xv_k,k]$ is called the \emph{query} vector for position $k$ in the input sequence
%  \mpitem $K_j[\xv_j,j]$ is called the \emph{key} vector for position $j$ in the input sequence
%  \mpitem $V_j[\xv_j]$ is called the \emph{value} vector for position $j$ in the input sequence
%\end{itemize}

\vspace{1em}

\maybepause
The query $Q_k$ can be a sum of \emph{all vectors supported in the attention sum}:\\
$Q_k = \xv_k + \xv_{m_1} + \cdots + \xv_{m_k}$\\
$\Rightarrow (Q_k^T \xv_j)\xv_j \approx \xv_j$, if $\xv_j$ is similar to \emph{any vector} in the query sum.

%We still need a reason to generalize $\xv$ above to a \emph{key} $K(\xv)$ and \emph{value} $V(\xv)$

\end{slide}

%% \begin{slide}[\slideopts]{Queries, Keys, and Values}

%% \begin{itemize}
%%   A query $Q_k$ formed as a sum of sought vectors, looks useful, but we can generalize further
%%   \mpitem The query for a \emph{noun} can look for \emph{all verbs} and \emph{all adjectives} that could contextualize it
%%   \mpitem The model space can have different partitions for nouns, verbs, and adjectives
%%   \mpitem We can learn a projection $K(\xv)$ that projects any $\xv$ to a vector representing ``noun'', ``verb'', or ``adjective,'' etc.
%%   \mpitem Finally, instead of adding the original vector into the sum, we can learn a different \emph{value vector} $V(\xv)$
%%   for building attention-sums in some new sector of the model space, etc.
%% \end{itemize}

%% So now we're up to (Q,K,V) attention:
%% \[
%% \cv_{kj}\xv_j = (Q_k^T K_j)V_j
%% \]

%% %Similarly, the key vector $K_j$ can be a sum of \emph{all queries} to which it applies, \eg,\\
%% %$K_j = \xv_j + \xv_{n_1} + \cdots + \xv_{n_k} \Rightarrow (Q_k^T K_j)\xv_j \approx M\xv_j$, given $M$ similar vectors in both.

  
%% %% \textbf{Idea:} For more flexibility in the attention sum (or to learn
%% %% down-projections in \emph{multi-head} mode), replace $\xv_j$ in the
%% %% attention sum with a learned \emph{value vector} $V_j[\xv_j]$\\

%% \end{slide}

\begin{slide}[\slideopts]{Multi-Head Attention}

\textbf{Idea:} To support multiple meaning possibilities, \emph{partition the model space} into\\
parallel independent \emph{attention calculations} (``multi-head attention'')

%\myFigureToWidth{one-pole-rnn-seq}{0.6\twidth}{} % {Sequence Modeling with Gated RNNs}
%\vspace{-4em}

\begin{itemize}
  \mpitem Each \emph{attention head} can form an independent input interpretation
  \mpitem Useful for \emph{ambiguous} sequences, especially in the lower layers
  \mpitem Also introduced in the Transformer paper (2017)
\end{itemize}

\maybepause

Now we need \emph{down-projections} for the query $Q(\xv)$, \emph{key} $K(\xv)$, and \emph{value} $V(\xv)$

\maybepause Relevance of input $j$ to output $k$ in attention-head $l$
is now proportional to
\[
\left[Q_{k,l}^T(\xv_k) K_{j,l}(\xv_j)\right]V_{j,l}(\xv_j)
\]
where $Q$, $K$, and $V$ are learned down-projections of the input
$\xv$ for each attention-head $l$ and for all sequence indices $j$ and
$k$ in the context buffer (``Transformer'')

Other useful generalizations can be imagined for these learned
vectors, such as grouping grammatical functions, creating new
model-space regions, etc.

\end{slide}

% Relatively weak/obvious points:
%% \begin{slide}[\slideopts]{Weight Tying}

%%   When the sequence model maps input to output in the same ``language'' (e.g., English to English),
%%   it makes sense to use the \emph{same embedding vectors} at the input and output layers, instead
%%   of separately learning a set of weights for mapping to the final output.
%%   This is called ``weight tying'' (many fewer parameters, better results).

%% \end{slide}

%% \begin{slide}[\slideopts]{Hierarchical Blocks}
%% \begin{itemize}
%%   \mpitem Cascade blocks of attention + MLP and/or gated recurrence + MLP to model \emph{hierarchical relationships} like image features or grammatical constructs
%%   \mpitem Attention and gated RNNs are called ``mixing layers'' (successive inputs are combined)
%%   \mpitem MLPs are called ``point transformations'' (general mapping of any vector from one place to another)
%%   \mpitem RMSNorm typical at the input to put it on the hypersphere --- also used internally\\
%%   (see Hawk/Griffin e.g.)
%%   \mpitem Thus, these ``point MLPs'' are effectively data-dependent \emph{rotation matrices}.\\
%%   In model-dimension $N$, we need only learn $N-1$ \emph{rotation angles} for each input class.\\
%%   The direct mapping of the coordinates used now seems efficient enough, but projection back to the sphere
%%   during training might help.  Then perhaps there is no need for RMSNorm at the output after summing the skip
%%   connection.
%% \end{itemize}
%% \end{slide}

%% \begin{slide}[\slideopts,toc={Multiple Heads}]{MultiHead Attention}

%% \vspace{-1em}

%% \textbf{Idea:} To enable several independent meaning possibilities, \emph{partition the space}

%% %\myFigureToWidth{one-pole-rnn-seq}{0.6\twidth}{} % {Sequence Modeling with Gated RNNs}
%% %\vspace{-4em}

%% \begin{itemize}
%%   \mpitem Each \emph{head} can form an independent sequence model
%%   \mpitem Filter coefficients computed from input as before, but smaller
%%   \mpitem Part of the \emph{Transformer} for sequence modeling (attention layer)
%% \end{itemize}

%% \end{slide}

%% \begin{slide}[\slideopts,toc={Other Features}]{Other ``Obvious'' Features}
%% \begin{itemize}
%%   \mpitem Tying output ``language modeling head'' weights to the input embedding weights
%%   \mpitem Positional encoding within an RNN
%% \end{itemize}
%% Less obvious:
%% \begin{itemize}
%%   \mpitem Multihead Attention (down-projection + independent spatial processing)
%%   \mpitem (Q,K,V) matrices for ``dot-product attention'' (down-projection + inner-product function + value flexibility)
%% \end{itemize}
%% \end{slide}

%% \begin{wideslidewhite}[\slideopts,toc={Architectures}]{Architectures}
%% \vspace{-4em}
%% \myFigureToWidth{Architectures}{\twidth}{}
%% \vspace{-4em}
%% \end{wideslidewhite}

\begin{slide}[\slideopts,toc={Hypersphere}]{Where Meaning Lies}
\vspace{-1em}
\myFigureToWidth{ginaSphere1}{\twidth}{}
\end{slide}

\section[\sectopts,toc={History Samples}]{Sequence Modeling Snapshots}

\begin{slidewhite}[\slideopts, toc={LSTM \& GRU}]{LSTM and GRU}
\vspace{-6em}
\myFigureRotateToWidth{Architectures1}{-90}{\twidth}{}
\end{slidewhite}

\begin{slidewhite}[\slideopts, toc={SSM \& Mamba}]{Structured State Space and Mamba}
\vspace{-6em}
\myFigureRotateToWidth{Architectures2}{-90}{\twidth}{}
\end{slidewhite}

\begin{slidewhite}[\slideopts,toc={Hawk \& Griffin}]{Hawk and Griffin}
\vspace{-6em}
\myFigureRotateToWidth{Architectures3}{-90}{\twidth}{}
\end{slidewhite}

\begin{slidewhite}[\slideopts,toc={HGRN2}]{Gated ``Linear'' RNNs with State Expansion}
\vspace{-6em}
\myFigureRotateToWidth{Architectures4}{-90}{\twidth}{}
\end{slidewhite}

\begin{slidewhite}[\slideopts,toc={RWKV+}]{RWKV, Eagle, Finch}
\vspace{-6em}
\myFigureRotateToWidth{Architectures5}{-90}{\twidth}{}
\end{slidewhite}

%\begin{slidewhite}[\slideopts,toc={}]{Architectures 6}
%\vspace{-6em}
%\myFigureRotateToWidth{Architectures6}{-90}{\twidth}{}
%\end{slidewhite}

%\input ai-reading-2022.tex
%\input ai-reading-2023.tex
%\input ai-projects.tex
%\section[\sectopts]{Differentiable DSP (DDSP)}
%\input ddsp.tex

\end{document}
\endinput
