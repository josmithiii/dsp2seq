%&latex
% Created from /l/mt/JOSOverview.tex

\newcommand{\theTitle}{Sequence Models from a Signal Processing Perspective}
\newcommand{\theEvent}{\htmladdnormallink{West Coast Machine Learning}{https://www.youtube.com/channel/UCuoNQWLuEYwjP7mI23sZ3WQ}} 
\newcommand{\theDate}{June 13, 2024}

%% \newcommand{\theTitle}{Inventing Modern Sequence Models as a Music 320 Project}
%% \newcommand{\theSubTitle}{Samples become Meaning Vectors}
%% \newcommand{\theEvent}{CCRMA Open House} 
%% \newcommand{\theDate}{May 17, 2024}
%% % Classroom (Knoll 217), Friday 1:40-1:55pm}

\newcommand{\theAuthor}{\htmladdnormallink{Julius Smith}{http://ccrma.stanford.edu/~jos/}}

%Mohonk05 said:
\input ../latex/stdpreshdr.tex % /w/latex/stdpreshdr.tex
\input ../latex/wgtmac.tex % /w/latex/wgtmac.tex

\usepackage{xcolor}
%N:\usepackage[dvipsnames]{xcolor}
%N:\usepackage[svgnames]{xcolor}

% Uses package etoolbox for \newtoggle et al:
\usepackage{etoolbox} % for \newtoggle et al
\newtoggle{local}
%\toggletrue{local}
\togglefalse{local}
\input localremote.tex

\date{\theDate}

\title{\theTitle}
\author{\theAuthor\\
%  CCRMA Open House\\
%  Classroom (Knoll 217)\\
%  Stanford University \\[10pt]
  \theEvent
}

\begin{document}

\maketitle

\section[\sectopts]{Background}

\begin{slide}[\slideopts,toc={Path to CCRMA}]{My Path to
    \htmladdnormallink{\textbf{CCRMA}}{http://ccrma.stanford.edu/}
    (Center for Computer Research in Music and Acoustics)
}

\centerline{Musician : Math : Physics : EE : Control : DSP : System ID : SAIL/CCRMA}
\vspace{1em}

\myTwoFiguresToBoxes{Julius}{jtm45_new2}{0.4\twidth}{0.6\textheight}{Some Gig}{Tube Amp}{}
%\myTwoFiguresToBoxes{Bittersweet2}{jtm45_new2}{0.4\twidth}{0.6\textheight}{Some Gig}{Tube Amp}{}
%\myTwoFiguresToBoxes{Julius}{Bittersweet2}{0.4\twidth}{0.6\textheight}{High School Band}{Tube Amp}{}
%\myTwoFiguresToBoxes{Julius}{moogschemsmall}{0.4\twidth}{0.6\textheight}{Some Gig}{Moog VCF Ladder}{}
%\myTwoFiguresToWidth{Julius}{wdelt}{0.4\twidth}{}{}{}
%\myTwoFiguresToWidth{Julius}{conicalcap2}{0.5\twidth}{Tesseract Gig}{Cone Proof}{}
%\myFigureRotateToWidth{Julius}{-90}{0.9\twidth}{}
%\centerline{JOS in situ}
\end{slide}

\input courses-overview.tex

%\end{document}
%\endinput

%\input abstract.tex

\section[\sectopts,toc={Basic Idea}]{Music 320 Project Idea}

\begin{slide}[\slideopts,toc={One Pole Filter}]{Assignment: Do Something Cool with a \emph{One-Pole Recursive Digital Filter}}
\vspace{-1em}
\myFigureToWidth{one-pole-jos}{0.5\twidth}{
  One Pole at $z = p$\\[5pt]
    $y(n) = g\, x(n) + p\, y(n-1)$, $n=0,1,2,\ldots$\\[5pt]
    $H(z) = \frac{\displaystyle Y(z)}{\displaystyle X(z)} = \frac{\displaystyle  g}{\displaystyle 1 - p\,\zi}$}
\maybepause
\vspace{-1em}
\textbf{Idea: Let's Make an Associative Memory!}
\begin{itemize}
\mpitem $x(n)$ can be a \emph{long vector} $\xv(n)\in\RN$ representing \emph{anything we want} --- any ``label''
\mpitem Set $\geev = 1$ and $\peev = -1$ to make $\yv(n)$ a \emph{sum of all input vectors} (``integrator'')
\mpitem Choose the dimension $N$ so large that \emph{vectors in the sum are mostly orthogonal}
\mpitem Retrieve similar vectors using a \emph{matched inner product} $\wv^T\xv > b$,\\
        for some suitable threshold $b$ (Hey! That's a simulated neuron! (``Perceptron''))
\end{itemize}
\end{slide}

\begin{slide}[\slideopts,toc={Inner Product}]{Vector Retrieval by Inner Product}

  Given the sum of vectors
  \[
  \yv(n) = \sum_{m=0}^n \xv(m)
  \]
  and a ``query vector'' $\wv = \xv(k)$,\\
  \maybepause
  find the query in the sum using an \emph{inner product:}
  \[
  \wv^T\yv(n) \eqsp \sum_{m=0}^n \wv^T\xv(m) \;\approx\; \xv^T(k)\,\xv(k) \eqsp \|\xv(k)\|^2 \;>\; b(k)
  \]
  where $b(k)$ is the \emph{detection threshold} for $\xv(k)$

  \begin{itemize}
  \mpitem This works because the spatial dimension is so large that $\xv^T(j)\,\xv(k)\approx \epsilon$ for $j\ne k$

  \mpitem Retrieval threshold $b(k)$ depends on $\|\xv(k)\|^2$\\
  $\Rightarrow$ \textbf{suggestion:} \emph{reserve the radial dimension for similarity scoring}

  \mpitem \Ie, \emph{only populate the \textbf{hypersphere}} in $\RN$: $\norm{\xv(k)} = 1, \, \forall k$

  \mpitem We just invented \textbf{\texttt{RMSNorm}}, used extensively in neural networks (not \texttt{LayerNorm})

  \end{itemize}

\end{slide}

%---------------------------------------------------------------------------------------------------

\begin{slide}[\slideopts,toc={Orthogonality}]{Orthogonality in High Dimensions}
\vspace{-1em}
% \input orthogonality.tex
Let $\ab\in\reals^N$ and$\bb\in\reals^N$ be two normally random, real, unit-norm vectors in $N$ dimensions.
$\|\ab\|=\|\bb\|=1$.

The dot-product of
$\ab^T=[a_1,a_2,\ldots,a_N]$ and
$\bb^T=[b_1,b_2,\ldots,b_N]$ is defined as
\[
\ab \cdot \bb = \sum_{i=1}^{N} a_i b_i.
\]

The squared dot product is
\[
(\ab \cdot \bb)^2 = \left(\sum_{i=1}^{N} a_i b_i\right)^2 = \sum_{i=1}^{N} \sum_{j=1}^{N} a_i a_j b_i b_j.
\]

Expected value (average):
\[
E\left[(\ab \cdot \bb)^2\right]
= \sum_{i=1}^{N} \sum_{j=1}^{N} E[a_i a_j] \, E[b_i b_j]
= \sum_{i=1}^{N} \frac{1}{N}\,\frac{1}{N}
= \zbox{\frac{1}{N}}
\]
\end{slide}

\begin{slide}[\slideopts,toc={}]{Orthogonality in High Dimensions, Continued}

We just showed
\[
E\left[(\ab \cdot \bb)^2\right] = \zbox{\frac{1}{N}}
\]
since $E[a_i b_j]=0$ for $i \ne j$, $E[a_i^2] = E[b_i]^2 = 1/N$, and $\ab$ and $\bb$ are independent.\\
In other words, the \emph{expected squared dot product of two normally random unit vectors in $\reals^N$ is $1/N$}.\\
\textbf{Suggestion:} \emph{Initialize biases larger than $1/N$}.\\
\textbf{Suggestion:} \emph{Divide the sum of $M$ vectors by $\sqrt{M}$:}
\begin{itemize}
  \item ``power normalization''
  \mpitem ``\tx{RMSNorm}-preserving''
  \mpitem Done in \htmladdnormallink{Hawk \& Griffin}{https://arxiv.org/abs/2402.19427}, \eg
  \mpitem ``Keep vector sums near the unit sphere''
\end{itemize}
\maybepause
\textbf{Suggestion:} Apply \tx{RMSNorm} when \emph{training} the initial \emph{vocabulary embedding}
\textbf{Suggestion:} Modify \emph{model dimension} according to \emph{layer width} at each level
% Note to self: embeddings have semantic proximity while feature maps may not

\end{slide}

\begin{slide}[\slideopts,toc={Model Dimension}]{Model Dimension vs.~Model Size
(says \htmladdnormallink{ChatGPT-4o}{https://chatgpt.com/share/76499bbc-95f7-48bf-8646-19786c1961f4}, not checked)}

\vspace{-2em}

% \usepackage{multicol}
% \usepackage{amsmath}

% \input ModelDims.tex

\begin{multicols}{2}
\begin{itemize}
    \item \textbf{Original Transformer}
    \begin{itemize}
      \item $d = 512$ ($65$ M, $VS = 50$ K?)
    \end{itemize}
    
    \item \textbf{BERT}
    \begin{itemize}
        \item $d = 768$ ($110$ M, $30$ K VS)
        \item $d = 1024$ ($340$ M, $30$ K VS)
    \end{itemize}
    
    \item \textbf{GPT-2}
    \begin{itemize}
        \item $d = 768$ ($124$ M, $50$ K VS)
        \item $d = 1024$ ($345$ M, $50$ K VS)
        \item $d = 1280$ ($774$ M, $50$ K VS)
        \item $d = 1600$ ($1.5$ B, $50$ K VS)
    \end{itemize}
    
    \item \textbf{GPT-3}
    \begin{itemize}
        \item $d = 2048$ ($2.7$ B, $50$ K VS)
        \item $d = 4096$ ($6.7$ B, $50$ K VS)
        \item $d = 6144$ ($13$ B, $50$ K VS)
        \item $d = 12288$ ($175$ B, $50$ K VS)
    \end{itemize}
    
    \item \textbf{T5}
    \begin{itemize}
        \item $d = 512$ ($60$ M, $32$ K VS)
        \item $d = 768$ ($220$ M, $32$ K VS)
        \item $d = 1024$ ($770$ M, $32$ K VS)
        \item $d = 1024$ ($3$ B, $32$ K VS)
        \item $d = 1024$ ($11$ B, $32$ K VS)
    \end{itemize}
    
    %% \item \textbf{RoBERTa}
    %% \begin{itemize}
    %%     \item $d = 768$ ($125$ M, $50$ K VS)
    %%     \item $d = 1024$ ($355$ M, $50$ K VS)
    %% \end{itemize}
    
    \item \textbf{ALBERT}
    \begin{itemize}
        \item $d = 768$ ($12$ M, $30$ K VS)
        \item $d = 1024$ ($18$ M, $30$ K VS)
        \item $d = 2048$ ($60$ M, $30$ K VS)
        \item $d = 4096$ ($235$ M, $30$ K VS)
    \end{itemize}
    
    \item \textbf{DistilBERT}
    \begin{itemize}
        \item $d = 768$ ($66$ M, $30$ K VS)
    \end{itemize}
    
    \item \textbf{Megatron-Turing NLG}
    \begin{itemize}
        \item $d = 20480$ ($530$ B, $50$ K VS)
    \end{itemize}
    
\end{itemize}
\end{multicols}

\end{slide}

\begin{slidewhite}[\slideopts,toc={}]{Model Dimension $d$ versus Log10 Model Size}

\vspace{-2em}

\myFigureToWidth{Model_Dimension_vs_Log10_Parameters}{\twidth}{}
  
\end{slidewhite}

%---------------------------------------------------------------------------------------------------

\section[\sectopts,toc={Architectures}]{Architectures}

\begin{slide}[\slideopts,toc={Vector Memory}]{Cumulative Vector Memory}
\vspace{-1em}
\myFigureToWidth{integrator}{0.6\twidth}{}
\vspace{-1em}
\myFigureToWidth{ginaSphere1}{0.5\twidth}{}
%\myTwoFiguresToWidth{integrator}{ginaSphere1}{0.5\twidth}{Vector summer}{MidJourney depiction for $N=3$}{}
\end{slide}

\begin{slide}[\slideopts,toc={Gating}]{Gated Vector Memory}

\myFigureToWidth{integrator}{0.6\twidth}{Input Vector Summer}

\begin{itemize}
\item \textbf{Problem:} Need a \emph{memory reset}
\mpitem \textbf{Solution:} Set \emph{feedback gain to zero} for one step to clear the memory
\item[]
\mpitem \textbf{Problem:} Need an \emph{input gate} to suppress unimportant inputs
\mpitem \textbf{Solution:} Set \emph{input gain to zero} for unimportant inputs
\item[]
\mpitem We just invented \textbf{gating}, used extensively in neural sequence models
\end{itemize}

\end{slide}

\begin{slide}[\slideopts,toc={Gated RNN}]{Gated Recurrent Network}

\vspace{-1em}

  \textbf{Idea:} \emph{Learn} the input and feedback gates as functions of input $\xv_n$\\
  based on many input-output examples $(\xv_n,\yv_n)$ (``training data''):
%% \mpitem Some item
%% \end{itemize}

\vspace{-1em}

\myFigureToWidth{one-pole-rnn}{0.4\twidth}{Vector Memory with Learned Input and Feedback Gates}

\vspace{-1em}

\maybepause

\textbf{Suggestions:}
\begin{itemize}
\mpitem Use learned, input-based, \emph{activations} for gating (LSTM, GRU, Mamba smoothed input)
\mpitem While activated, \emph{optionally} set \emph{memory duration} via $\peev$ magnitude (SSMs, Mamba)
\begin{itemize}
  \mpitem \emph{Initialize} $\peev$ for desired initial memory duration (exponential fade time)
  \mpitem Learn $\peev(\xv_n)$ as $\Imtx\cdot e^{-\Delta}\approx \Imtx -\Imtx \Delta$,
  %and $\geev(\xv_n)$ as $\mbox{Linear}(\xv_n,\yv_n)\cdot\Delta$,
  where $\Delta = \mbox{softPlus}(\mbox{parameter}(\xv_n,\Yv_n))$ (guaranteed stable --- no ``exploding gradients'')
  [Also multiply $\geev(\xv_n)$ by $\Delta$] % for gain-normalization]
%\mpitem \emph{memory duration} $\Rightarrow$ \emph{linear projection}\\
  % (SSMs, Mamba) ---
%  \mpitem SSMs led to \emph{linear projection} (\emph{no activation} in feedback)
%  based on cool \emph{history-polynomial-approximation} ideas that ultimately went away (S4D), but \emph{linearity survives}.\\
  \mpitem Consider \emph{separate meaning-driven activation} multiplying feedback: $\sigma(\Lmtx\xv)\peev(\xv)$
%  Example: activation by end-of-sentence detector, end-of-paragraph, etc. (e.g., for RAG sentence/paragraph embeddings)\\
%  Analogy: clear Transformer input context buffer after processing a full sentence/paragraph
\end{itemize}
\end{itemize}

\end{slide}

\begin{slide}[\slideopts,toc={Skip Connection}]{Output Gating}

  \vspace{-1em}

  \textbf{Idea:} Since we have input and feedback gates, why not an \textbf{\emph{output gate} and bypass?}

\myFigureToWidth{one-pole-rnn-skip}{0.6\twidth}{Gated RNN with \textbf{Skip Connection}}

\maybepause
Output gating allows network to be ``bypassed'' when not helpful.

\begin{itemize}
\mpitem \textbf{``Obvious'' Suggestion:} The bypass path should be scaled for \emph{power normalization}\\
\mpitem \textbf{Better yet:} Don't scale the bypass and use \tx{RMSNorm} at the input of the next layer\\
(prevents a ``bad layer'' from isolating deeper layers from the input with garbage)
\end{itemize}

\end{slide}

\begin{slide}[\slideopts]{State Expansion}

\vspace{-1em}

  \textbf{Idea:} \emph{Expand} vector-memory dimension to an integer multiple of the model dimension:
% \vspace{-4em}
% \myFigureToWidth{one-pole-rnn-seq}{0.6\twidth}{} % {Sequence Modeling with Gated RNNs}
%

\myFigureToWidth{statespace-rnn}{0.5\twidth}{}

\vspace{-2em}

\begin{itemize}
  \mpitem Increased storage capacity
  \mpitem Multiple parallel memories, as in ``multi-head attention'' with individual durations
  \mpitem \emph{``Structured State-Space Models''} (SSM) look like this (see Mamba, \eg)
  \mpitem Feedback matrix $\Amtx$ typically \emph{diagonal} since 2022 (see ``S4D'')\\
  	  $\Rightarrow$ Parallel bank of vector one-poles (\emph{``linearly'' gated, state-expanded RNNs})
  % \mpitem Processed sequence (``context buffer'') is \emph{indefinitely long}
  \mpitem Gating matrices in Mamba are simple linear input projections: 
%  \[
  $
          [\Bmtx(\xv_n), \Cmtx(\xv_n)] = \Lmtx\,\xv_n  % , \qquad \Dmtx(\xv_n) = \mbox{SiLU}(\Lmtx^\prime\xv_n)\]
  $
%  \] 
  % (See Mamba, \eg)
  % \mpitem Conv1D mixing followed by SiLU on input for Mamba (only nonlinear activation)
\end{itemize}

\end{slide}

\section[\sectopts,toc={Processing}]{Memory Access}

\begin{slide}[\slideopts,toc={Perceptrons}]{Detecting Multiple Vectors in Parallel}
\maybepause
\vspace{1em}
\textbf{Idea:} Detect multiple memory vectors in parallel using an array of \emph{``Perceptrons''}
%\emph{``Multi-Layer Perceptron'' (MLP)}
\begin{itemize}
\mpitem Each Perceptron detects one or more memory vectors similar to its weight vectors
\mpitem The Perceptrons indicate \emph{which weight-vectors are present} in the vector memory
\end{itemize}

\end{slide}

\begin{slide}[\slideopts,toc={Sequences}]{Sequence Modeling}
  \vspace{-1em}
  \begin{itemize}
    \mpitem If each vector represents a \emph{word,} a vector sum is simply a \emph{bag of words}
    \mpitem To model a \emph{sequence} of words, we have various \emph{sequence-position-encoding} options:
    \begin{enumerate}
      \mpitem \emph{Amplitude Decay} - Multiply the sum by a \emph{forgetting factor} each sequence step\\
      (RNNs) - \emph{poor choice} (conflates with angular distance on the hypersphere)
      \mpitem \emph{Sinusoidal Amplitude Modulation} - Add a sinusoid with \emph{increasing frequency} to each vector summing into the history\\
      (used in the original Transformer)
      \mpitem \emph{Phase Shift} - Multiply by the sum by $e^{j\Delta}$ each sample\\
      (``RoPE'') - \emph{apparently most used today}
    \end{enumerate}
    % Toward Xformer: \mpitem Use many vector-sum memories in parallel, positionally encoded (``State Expansion'' in SSMs)
    % \mpitem Learn position-specific \emph{FIR Coefficients} across a parallel bank of positionally encoded vector-sum memories (``Attention Layer'' in a Transformer)
  \end{itemize}
\end{slide}

\section[\sectopts,toc={Attention}]{Attention}

\begin{slide}[\slideopts,toc={Attention}]{Attention Layer}

\vspace{-1em}

\textbf{Idea:} Also use \emph{FIR Filtering}

%\myFigureToWidth{one-pole-rnn-seq}{0.6\twidth}{} % {Sequence Modeling with Gated RNNs}
%\vspace{-4em}

\myFigureToWidth{attention}{0.8\twidth}{}

\vspace{-1em}

We can have \emph{separately learned FIR coefficient matrices} $b_j[\xv(n-j),k]$, \\
  which depend on the
  \begin{enumerate}
    \mpitem input \emph{position} $j$ in the input sequence (``context buffer'')
    \mpitem input \emph{vector} $\xv(n-j)$, $j=0,1,2,\dots,M$, (nonlinear)
    \mpitem \emph{output-position} $k$ being computed, $k=0,1,2,\dots,M$ ($M+1$ outputs)
  \end{enumerate}

\maybepause
\textbf{Idea:} Add \emph{relevance gating} suppressing unimportant inputs to each output (``attention'')

\vspace{0.5em}

\maybepause
\textbf{Idea:} Measure relevance using an \emph{inner product} between the output and input positions (``dot-product attention'')
\end{slide}

\begin{slide}[\slideopts]{Dot-Product Attention}

\vspace{-1em}

\myFigureToWidth{attention}{0.8\twidth}{}

\vspace{-1em}

\textbf{Relevance Gating}

Contribution from input $\xv(n-j)$ to the nonlinear FIR sum for output $\yv(n-k)$ is $(Q_k^T K_j)\xv(n-j)$, where
\begin{itemize}
  \mpitem $Q_k[\xv(n-k),k]$ is called the \emph{query} vector for position $k$ in the input sequence
  \mpitem $K_j[\xv(n-j),j]$ is called the \emph{key} vector for position $j$ in the input sequence
%  \mpitem $V_j[\xv(n-j)]$ is called the \emph{value} vector for position $j$ in the input sequence
\end{itemize}

\maybepause

\textbf{Idea:} To provide more flexibility for the attention sum,
replace $\xv(n-j)$ in the attention sum with a learned \emph{value
vector} $V_j[\xv(n-j)]$\\
%
\maybepause
$\Rightarrow$\\
%
Relevance of input $j$ to output $k$ $\propto (Q_k^T K_j)V_j$, where
all three vectors are learned projections of $\xv(n-j)$ for each
$(j,k)$ (``Transformer'')

\end{slide}

\begin{slide}[\slideopts]{Multi-Head Attention}

\textbf{Idea:} To support multiple meaning possibilities, \emph{partition the model space} into\\
parallel independent \emph{attention calculations} (``multi-head attention'')

%\myFigureToWidth{one-pole-rnn-seq}{0.6\twidth}{} % {Sequence Modeling with Gated RNNs}
%\vspace{-4em}

\begin{itemize}
  \mpitem Each \emph{attention head} can form an independent input interpretation
  \mpitem Useful for \emph{ambiguous} sequences, especially in the lower layers
  \mpitem Also introduced in the Transformer paper (2017)
\end{itemize}

\end{slide}

\begin{slide}[\slideopts]{Weight Tying}

  When the sequence model maps input to output in the same ``language'' (e.g., English to English),
  it makes sense to use the \emph{same embedding vectors} at the input and output layers, instead
  of separately learning a set of weights for mapping to the final output.
  This is called ``weight tying'' (many fewer parameters, better results).

\end{slide}

\begin{slide}[\slideopts]{Hierarchical Blocks}
\begin{itemize}
  \mpitem Cascade blocks of attention + MLP and/or gated recurrence + MLP to model \emph{hierarchical relationships} like image features or grammatical constructs
  \mpitem Attention and gated RNNs are called ``mixing layers'' (successive inputs are combined)
  \mpitem MLPs are called ``point transformations'' (general mapping of any vector from one place to another)
  \mpitem RMSNorm typical at the input to put it on the hypersphere --- also used internally (see Hawk/Griffin e.g.)
  \mpitem Thus, these point MLPs are effectively data-dependent \emph{rotation matrices}.\\
  In model-dimension $N$, we need only learn $N-1$ \emph{rotation angles} for each input class.\\
  The direct mapping of the coordinates used now seems efficient enough, but projection back to the sphere
  during training might help.  Then perhaps there is no need for RMSNorm at the output after summing the skip
  connection.
\end{itemize}
\end{slide}

%% \begin{slide}[\slideopts,toc={Multiple Heads}]{MultiHead Attention}

%% \vspace{-1em}

%% \textbf{Idea:} To enable several independent meaning possibilities, \emph{partition the space}

%% %\myFigureToWidth{one-pole-rnn-seq}{0.6\twidth}{} % {Sequence Modeling with Gated RNNs}
%% %\vspace{-4em}

%% \begin{itemize}
%%   \mpitem Each \emph{head} can form an independent sequence model
%%   \mpitem Filter coefficients computed from input as before, but smaller
%%   \mpitem Part of the \emph{Transformer} for sequence modeling (attention layer)
%% \end{itemize}

%% \end{slide}

%% \begin{slide}[\slideopts,toc={Other Features}]{Other ``Obvious'' Features}
%% \begin{itemize}
%%   \mpitem Tying output ``language modeling head'' weights to the input embedding weights
%%   \mpitem Positional encoding within an RNN
%% \end{itemize}
%% Less obvious:
%% \begin{itemize}
%%   \mpitem Multihead Attention (down-projection + independent spatial processing)
%%   \mpitem (Q,K,V) matrices for ``dot-product attention'' (down-projection + inner-product function + value flexibility)
%% \end{itemize}
%% \end{slide}

%% \begin{wideslidewhite}[\slideopts,toc={Architectures}]{Architectures}
%% \vspace{-4em}
%% \myFigureToWidth{Architectures}{\twidth}{}
%% \vspace{-4em}
%% \end{wideslidewhite}

\begin{slide}[\slideopts,toc={Hypersphere}]{Where Meaning Lies}
\vspace{-1em}
\myFigureToWidth{ginaSphere1}{\twidth}{}
\end{slide}

\section[\sectopts,toc={History Samples}]{Sequence Modeling Snapshots}

\begin{slidewhite}[\slideopts, toc={LSTM \& GRU}]{LSTM and GRU}
\vspace{-6em}
\myFigureRotateToWidth{Architectures1}{-90}{\twidth}{}
\end{slidewhite}

\begin{slidewhite}[\slideopts, toc={SSM \& Mamba}]{Structured State Space and Mamba}
\vspace{-6em}
\myFigureRotateToWidth{Architectures2}{-90}{\twidth}{}
\end{slidewhite}

\begin{slidewhite}[\slideopts,toc={Hawk \& Griffin}]{Hawk and Griffin}
\vspace{-6em}
\myFigureRotateToWidth{Architectures3}{-90}{\twidth}{}
\end{slidewhite}

\begin{slidewhite}[\slideopts,toc={HGRN2}]{Gated ``Linear'' RNNs with State Expansion}
\vspace{-6em}
\myFigureRotateToWidth{Architectures4}{-90}{\twidth}{}
\end{slidewhite}

\begin{slidewhite}[\slideopts,toc={RWKV+}]{RWKV, Eagle, Finch}
\vspace{-6em}
\myFigureRotateToWidth{Architectures5}{-90}{\twidth}{}
\end{slidewhite}

%\begin{slidewhite}[\slideopts,toc={}]{Architectures 6}
%\vspace{-6em}
%\myFigureRotateToWidth{Architectures6}{-90}{\twidth}{}
%\end{slidewhite}

%\input ai-reading-2022.tex
%\input ai-reading-2023.tex
%\input ai-projects.tex
%\section[\sectopts]{Differentiable DSP (DDSP)}
%\input ddsp.tex

\end{document}
\endinput
