%&latex
% Created from /l/mt/JOSOverview.tex

\newcommand{\theTitle}{Inventing Sequence Models as Vectorized Signal Processors}
%\newcommand{\theTitle}{Sequence Models from a Signal Processing Perspective}
\newcommand{\theEvent}{\htmladdnormallink{West Coast Machine Learning}{https://www.youtube.com/channel/UCuoNQWLuEYwjP7mI23sZ3WQ}}
\newcommand{\theDate}{June 20, 2024}

%% \newcommand{\theTitle}{Inventing Modern Sequence Models as a Music 320 Project}
%% \newcommand{\theSubTitle}{Samples become Meaning Vectors}
%% \newcommand{\theEvent}{CCRMA Open House}
%% \newcommand{\theDate}{May 17, 2024}
%% % Classroom (Knoll 217), Friday 1:40-1:55pm}

\newcommand{\theAuthor}{\htmladdnormallink{Julius Smith}{http://ccrma.stanford.edu/~jos/}}

%Mohonk05 said:
\input ../latex/stdpreshdr.tex % /w/latex/stdpreshdr.tex
%\input ../latex/stddefs.tex % /w/latex/stddefs.tex
\input ../latex/wgtmac.tex % /w/latex/wgtmac.tex

\usepackage{xcolor}
%N:\usepackage[dvipsnames]{xcolor}
%N:\usepackage[svgnames]{xcolor}

% Uses package etoolbox for \newtoggle et al:
\usepackage{etoolbox} % for \newtoggle et al
\newtoggle{local}
\toggletrue{local}
%\togglefalse{local}
\input localremote.tex

\date{\theDate}

\title{\theTitle}
\author{\theAuthor\\
%  CCRMA Open House\\
%  Classroom (Knoll 217)\\
%  Stanford University \\[10pt]
  \theEvent
}

\newcommand{\onevec}{\underline{1}}

\begin{document}

\maketitle

\input abstract.tex

\section[\sectopts]{Background}

\begin{slide}[\slideopts,toc={Path to CCRMA}]{My Path to
    \htmladdnormallink{\textbf{CCRMA}}{http://ccrma.stanford.edu/}
    (Center for Computer Research in Music and Acoustics)
}

\centerline{Musician : Math : Physics : EE : Control : DSP : System ID : SAIL/CCRMA}
\vspace{1em}

\myTwoFiguresToBoxes{Julius}{jtm45_new2}{0.4\twidth}{0.6\textheight}{Some Gig}{Tube Amp}{}
%\myTwoFiguresToBoxes{Bittersweet2}{jtm45_new2}{0.4\twidth}{0.6\textheight}{Some Gig}{Tube Amp}{}
%\myTwoFiguresToBoxes{Julius}{Bittersweet2}{0.4\twidth}{0.6\textheight}{High School Band}{Tube Amp}{}
%\myTwoFiguresToBoxes{Julius}{moogschemsmall}{0.4\twidth}{0.6\textheight}{Some Gig}{Moog VCF Ladder}{}
%\myTwoFiguresToWidth{Julius}{wdelt}{0.4\twidth}{}{}{}
%\myTwoFiguresToWidth{Julius}{conicalcap2}{0.5\twidth}{Tesseract Gig}{Cone Proof}{}
%\myFigureRotateToWidth{Julius}{-90}{0.9\twidth}{}
%\centerline{JOS in situ}
\end{slide}

\input courses-overview.tex

%\end{document}
%\endinput

\section[\sectopts,toc={Basic Idea}]{Music 320 Project Idea}

\begin{slide}[\slideopts,toc={One Pole Filter}]{Assignment: Do Something Cool with a \emph{One-Pole Recursive Digital Filter}}
\vspace{-1em}
\myFigureToWidth{one-pole-jos}{0.5\twidth}{
  One Pole at $z = p$\\[5pt]
    $y(n) = g\, x(n) + p\, y(n-1)$, $n=0,1,2,\ldots$\\[5pt]
    $H(z) = \frac{\displaystyle Y(z)}{\displaystyle X(z)} = \frac{\displaystyle  g}{\displaystyle 1 - p\,\zi}$}
\maybepause
\vspace{-1em}
\textbf{Idea: Let's Make an Associative Memory!}
\begin{itemize}
\mpitem $x(n)$ can be a \emph{long vector} $\xv(n)\in\RN$ representing \emph{anything we want} --- any ``label''
\mpitem Set $\geev = \onevec$ and $\peev = \onevec$ to make $\yv(n)$ a \emph{sum of all input vectors} (``integrator'')
\mpitem Choose the dimension $N$ so large that \emph{vectors in the sum are mostly orthogonal}
\mpitem Retrieve similar vectors using a \emph{matched inner product} $\wv^T\xv > b$,\\
        for some suitable threshold $b$ (Hey! That's a simulated neuron! (``Perceptron''))
\end{itemize}
\end{slide}

\begin{slide}[\slideopts,toc={Inner Product}]{Vector Retrieval by Inner Product}

  Given the sum of vectors
  \[
  \yv(n) = \sum_{m=0}^n \xv(m)
  \]
  and a ``query vector'' $\wv = \xv(k)$,\\
  \maybepause
  find the query in the sum using an \emph{inner product:}
  \[
  \wv^T\yv(n) \eqsp \sum_{m=0}^n \wv^T\xv(m) \;\approx\; \xv^T(k)\,\xv(k) \eqsp \|\xv(k)\|^2 \;>\; b(k)
  \]
  where $b(k)$ is the \emph{detection threshold} for $\xv(k)$

  \begin{itemize}
  \mpitem This works because the spatial dimension is so large that $\xv^T(j)\,\xv(k)\approx \epsilon$ for $j\ne k$

  \mpitem Retrieval threshold $b(k)$ depends on $\|\xv(k)\|^2$\\
  $\Rightarrow$ \textbf{suggestion:} \emph{reserve the radial dimension for similarity scoring}

  \mpitem \Ie, \emph{only populate the \textbf{hypersphere}} in $\RN$: $\norm{\xv(k)} = 1, \, \forall k$

  \mpitem We just invented \textbf{\texttt{RMSNorm}}, used extensively in neural networks (not \texttt{LayerNorm})

  \end{itemize}

\end{slide}

\begin{slide}[\slideopts,toc={}]{Decaying Vector Retrieval by Inner Product}

  RNNs typically have a \emph{forgetting factor} $p<1$.

  Given the sum of vectors
  \[
  \yv(n) = \sum_{m=0}^n p^{n-m}\xv(m)
  \]
  and a ``query vector'' $\wv = \xv(k)$,\\
  \maybepause
  the inner product now gives
  \[
  \wv^T\yv(n) \eqsp \sum_{m=0}^n \wv^Tp^{n-m}\xv(m) \;\approx\; p^{n-k}\xv^T(k)\,\xv(k) \eqsp p^{n-k} \;>\; b
  \]
  where $b$ is the detection threshold for $\xv(k)$, independent of $k$ since $\|\xv(k)\|=1$

  \begin{itemize}
  \mpitem \emph{Cannot retrieve} when $p^{n-k} < b$, setting an upper limit on $n$
  \mpitem We need $p > b^{1/n}$ or $n_{\mbox{max}} \le \log(b)/\log(p)$
  \mpitem Lowering $b$ extends the memory range, but admits more false detections due to ``interference''
  \mpitem ``Interference'' here means ``other vectors in the sum''
  \end{itemize}

\end{slide}

%---------------------------------------------------------------------------------------------------

\begin{slide}[\slideopts,toc={Orthogonality}]{Orthogonality in High Dimensions}
\vspace{-1em}
% \input orthogonality.tex
Let $\ab\in\reals^N$ and $\bb\in\reals^N$ be two normally random, real, unit-norm vectors in $N$ dimensions,
with $\|\ab\|=\|\bb\|=1$

\maybepause
The dot-product (inner product) of
$\ab^T=[a_1,a_2,\ldots,a_N]$ and
$\bb^T=[b_1,b_2,\ldots,b_N]$ is defined as
\[
\ab \cdot \bb = \ab^T\bb = \sum_{i=1}^{N} a_i b_i.
\]

\maybepause
The squared dot product is
\[
(\ab \cdot \bb)^2 = \left(\sum_{i=1}^{N} a_i b_i\right)^2 = \sum_{i=1}^{N} \sum_{j=1}^{N} a_i a_j b_i b_j.
\]

\maybepause
Expected value (average):
\[
E\left[(\ab \cdot \bb)^2\right]
= \sum_{i=1}^{N} \sum_{j=1}^{N} E[a_i a_j] \, E[b_i b_j]
= \sum_{i=1}^{N} \frac{1}{N}\,\frac{1}{N}
= \zbox{\frac{1}{N}}
\]
\end{slide}

\begin{slide}[\slideopts,toc={}]{Orthogonality in High Dimensions, Continued}
\vspace{-1em}
We just showed the \emph{expected squared dot product of two normally random unit vectors in $\reals^N$ is $1/N$}, \ie,
\[
E\left[(\ab \cdot \bb)^2\right] = \zbox{\frac{1}{N}}
\]
since $E[a_i b_j]=0$ for $i \ne j$, $E[a_i^2] = E[b_i]^2 = 1/N$, and $\ab$ and $\bb$ are independent.\\
\maybepause
\textbf{Suggestions:}
\begin{itemize}
\mpitem \emph{Initialize biases (detection thresholds) larger than $1/N$}
\mpitem \emph{Divide the sum of $M$ vectors by $\sqrt{M}$:}
\begin{itemize}
  \item ``power normalization''
  \mpitem ``\tx{RMSNorm}-preserving''
  \mpitem Done in \htmladdnormallink{Hawk \& Griffin}{https://arxiv.org/abs/2402.19427}, \eg
  \mpitem ``Keep vector sums near the unit sphere''
\end{itemize}
\mpitem Apply \tx{RMSNorm} when \emph{training} the initial \emph{vocabulary embedding} (``\tx{word2sphere}'')
\mpitem Set the \emph{model dimension} just sufficient for the \emph{layer width} at each level
\mpitem Caveat: We are only considering \emph{one mechanism} here --- \emph{others are definitely going on}
\end{itemize}
% Note to self: embeddings have semantic proximity while feature maps may not

\end{slide}

\begin{slide}[\slideopts,toc={}]{Orthogonality of Random Sums}
\vspace{-1em}
Similarly,
\beas
E\left[\left(\wv^T\yv_n\right)^2\right]
&=& E\left[\left(\sum_{m=0}^n \wv^T\xv_m\right)^2\right]
\eqsp \sum_{l=0}^n\sum_{m=0}^n E \left[ \wv^T\xv_l\xv_m^T\wv \right]\\[5pt]
&=& \sum_{m=0}^n E \left[ \wv^T\xv_m\xv_m^T\wv \right]
\eqsp \sum_{m=0}^n E\left[\left(\wv^T\xv_m\right)^2\right]
\eqsp \zbox{\frac{n}{N}}
\eeas
assuming $\wv\notin\yv$ and $\|\wv\|=\|\xv_m\|=1$ for all $m$. Thus,
\emph{retrieval becomes unreliable when the number of summed vectors $n$ nears the model dimension $N$.}
\begin{itemize}
  \mpitem $N$ is of course the number of exactly orthogonal vectors possible in $N$ dimensions
%  \mpitem We the importance of input and feedback \emph{gating}
%  \mpitem Later it will similarly be the importance of \emph{selective attention}
  \mpitem If $L$ vectors are in the sum, our Perceptron ``bias'' (detection threshold)
  should be higher than $L/N$ when there are $L$ vectors in the memory
  \mpitem \textbf{Suggestion:} \emph{Keep the number of active vectors
  in the memory well below the model dimension $N$}
\end{itemize}
\end{slide}

\begin{slide}[\slideopts,toc={}]{Orthogonality of Exponentially Decaying Random Sums}
\vspace{-1em}
RNNs typically have a \emph{forgetting factor} $p<1$, in which case we have,\\
defining $\mu=n-m$ and $\lambda=n-l$:
\beas
E\left[\left(\wv^T\yv_n\right)^2\right]
&=& E\left[\left(\sum_{m=0}^n \wv^Tp^\mu\xv_m\right)^2\right]
\eqsp \sum_{l=0}^n\sum_{m=0}^n E \left[ \wv^Tp^\lambda\xv_lp^\mu\xv_m^T\wv \right]\\[5pt]
&=& \sum_{m=0}^n p^{2\mu} E \left[ \wv^T\xv_m\xv_m^T\wv \right]
\eqsp \sum_{m=0}^n p^{2\mu} E\left[\left(\wv^T\xv_m\right)^2\right]\\[5pt]
&=& \zbox{\frac{1}{N} \frac{1-p^{2(n+1)}}{1-p^2}}
\;\to\; \frac{1}{N} \frac{1}{1-p^2}\quad\mbox{(as $n\to\infty$)}
\eeas
\begin{itemize}
  \mpitem For $1/(1-p^2) < N$, keep $p < \sqrt{(N-1)/N}$
  \mpitem For $1/(1-p^2) < N/2$, keep $p < \sqrt{(N-2)/N}$
\end{itemize}
and so on.  \emph{This gives us one way to calculate a maximum feedback coefficient $p$ in RNNs}
\end{slide}

\begin{slide}[\slideopts,toc={Model Dimension}]{Model Dimension vs.~Model \& Vocab Size
(\htmladdnormallink{ChatGPT-4o}{https://chatgpt.com/share/76499bbc-95f7-48bf-8646-19786c1961f4}, not checked)}

\vspace{-2em}

% \usepackage{multicol}
% \usepackage{amsmath}

% \input ModelDims.tex

\begin{multicols}{2}
\begin{itemize}
    \item \textbf{Original Transformer}
    \begin{itemize}
      \item $N = 512$ (Model Size $65$ M)
    \end{itemize}

    \item \textbf{BERT}
    \begin{itemize}
        \item $N = 768$ ($110$ M, $30$ K)
        \item $N = 1024$ ($340$ M, $30$ K)
    \end{itemize}

    \item \textbf{GPT-2}
    \begin{itemize}
        \item $N = 768$ ($124$ M, $50$ K)
        \item $N = 1024$ ($345$ M, $50$ K)
        \item $N = 1280$ ($774$ M, $50$ K)
        \item $N = 1600$ ($1.5$ B, $50$ K)
    \end{itemize}

    \item \textbf{GPT-3}
    \begin{itemize}
        \item $N = 2048$ ($2.7$ B, $50$ K)
        \item $N = 4096$ ($6.7$ B, $50$ K)
        \item $N = 6144$ ($13$ B, $50$ K)
        \item $N = 12288$ ($175$ B, $50$ K)
    \end{itemize}

    \item \textbf{T5}
    \begin{itemize}
        \item $N = 512$ ($60$ M, $32$ K)
        \item $N = 768$ ($220$ M, $32$ K)
        \item $N = 1024$ ($770$ M, $32$ K)
        \item $N = 1024$ ($3$ B, $32$ K)
        \item $N = 1024$ ($11$ B, $32$ K)
    \end{itemize}

    %% \item \textbf{RoBERTa}
    %% \begin{itemize}
    %%     \item $N = 768$ ($125$ M, $50$ K)
    %%     \item $N = 1024$ ($355$ M, $50$ K)
    %% \end{itemize}

    \item \textbf{ALBERT}
    \begin{itemize}
        \item $N = 768$ ($12$ M, $30$ K)
        \item $N = 1024$ ($18$ M, $30$ K)
        \item $N = 2048$ ($60$ M, $30$ K)
        \item $N = 4096$ ($235$ M, $30$ K)
    \end{itemize}

    \item \textbf{DistilBERT}
    \begin{itemize}
        \item $N = 768$ ($66$ M, $30$ K)
    \end{itemize}

    \item \textbf{Megatron-Turing NLG}
    \begin{itemize}
        \item $N = 20480$ ($530$ B, $50$ K)
    \end{itemize}

\end{itemize}
\end{multicols}

\end{slide}

\begin{slidewhite}[\slideopts,toc={}]{Log10 Model Dimension versus Log10 Model Size}

\vspace{-2em}

%\myFigureToWidth{Model_Dimension_vs_Log10_Parameters}{\twidth}{}
\myFigureToWidth{Model_Dimensions_vs_Model_Sizes_Labeled}{\twidth}{}

\end{slidewhite}

%---------------------------------------------------------------------------------------------------

\section[\sectopts,toc={Architectures}]{Architectures}

\begin{slide}[\slideopts,toc={Vector Memory}]{Cumulative Vector Memory}
\vspace{-1em}
\myFigureToWidth{integrator}{0.6\twidth}{}
\vspace{-1em}
\myFigureToWidth{ginaSphere1}{0.5\twidth}{MidJourney}
%\myTwoFiguresToWidth{integrator}{ginaSphere1}{0.5\twidth}{Vector summer}{MidJourney depiction for $N=3$}{}
\end{slide}

\begin{slide}[\slideopts,toc={Gating}]{Gated Vector Memory}

\myFigureToWidth{integrator}{0.6\twidth}{Input Vector Summer}

\begin{itemize}
\item \textbf{Problem:} Need a \emph{memory reset}
\mpitem \textbf{Solution:} Set \emph{feedback gain to zero} for one step to clear the memory
\item[]
\mpitem \textbf{Problem:} Need an \emph{input gate} to suppress unimportant inputs
\mpitem \textbf{Solution:} Set \emph{input gain to zero} for unimportant inputs
\item[]
\mpitem We just invented \textbf{gating}, used extensively in neural sequence models
\end{itemize}

\end{slide}

\begin{slide}[\slideopts,toc={Gated RNN}]{Gated Recurrent Network}

\vspace{-1em}

  \textbf{Idea:} \emph{Learn} the input and feedback gates as functions of input $\xv_n$\\
  based on many input-output examples $(\xv_n,\yv_n)$ (``training data''):
%% \mpitem Some item
%% \end{itemize}

\vspace{-1em}

\myFigureToWidth{one-pole-rnn}{0.4\twidth}{Vector Memory with Learned Input and Feedback Gates}

\vspace{-1em}

\maybepause

\textbf{Suggestions:}
\begin{itemize}
\mpitem Use learned, input-based, \emph{activations} for gating (LSTM, GRU, Mamba smoothed input)
\mpitem While activated, \emph{optionally} set \emph{memory duration} via $\peev$ magnitude (SSMs, Mamba)
\begin{itemize}
  \mpitem \emph{Initialize} $\peev$ for desired initial memory duration (exponential fade time)
  \mpitem Learn $\peev(\xv_n)$ as $\Imtx\cdot e^{-\Delta}\approx \Imtx -\Imtx \Delta$,
  %and $\geev(\xv_n)$ as $\mbox{Linear}(\xv_n,\yv_n)\cdot\Delta$,
  where $\Delta = \mbox{softPlus}(\mbox{parameter}(\xv_n,\yv_n))$ (guaranteed stable --- no ``exploding gradients'')
  [Also multiply $\geev(\xv_n)$ by $\Delta$] % for gain-normalization]
%\mpitem \emph{memory duration} $\Rightarrow$ \emph{linear projection}\\
  % (SSMs, Mamba) ---
%  \mpitem SSMs led to \emph{linear projection} (\emph{no activation} in feedback)
%  based on cool \emph{history-polynomial-approximation} ideas that ultimately went away (S4D), but \emph{linearity survives}.\\
  \mpitem Consider \emph{separate meaning-driven activation} multiplying feedback: $\sigma(\Lmtx\xv)\peev(\xv)$
%  Example: activation by end-of-sentence detector, end-of-paragraph, etc. (e.g., for RAG sentence/paragraph embeddings)\\
%  Analogy: clear Transformer input context buffer after processing a full sentence/paragraph
\end{itemize}
\end{itemize}

\end{slide}

\begin{slide}[\slideopts,toc={Skip Connection}]{Output Gating}

  \vspace{-1em}

  \textbf{Idea:} Since we have input and feedback gates, why not an \textbf{\emph{output gate} and bypass?}

\myFigureToWidth{one-pole-rnn-skip}{0.6\twidth}{Gated RNN with \textbf{Skip Connection}}

\maybepause
Output gating allows network to be ``bypassed'' when not helpful.

\begin{itemize}
\mpitem \textbf{``Obvious'' Suggestion:} The bypass path should be scaled for \emph{power normalization}\\
\mpitem \textbf{Better yet:} Don't scale the bypass and use \tx{RMSNorm} at the input of the next layer\\
(prevents a ``bad layer'' from isolating deeper layers from the input with garbage)
\end{itemize}

\end{slide}

\begin{slide}[\slideopts]{State Expansion}

\vspace{-1em}

  \textbf{Idea:} \emph{Expand} vector-memory dimension to an integer multiple of the model dimension:
% \vspace{-4em}
% \myFigureToWidth{one-pole-rnn-seq}{0.6\twidth}{} % {Sequence Modeling with Gated RNNs}
%

\myFigureToWidth{statespace-rnn}{0.5\twidth}{}

\vspace{-1em}

\maybepause

\emph{``Structured State-Space Models''} (SSM) look like this (\eg, Mamba)
\begin{itemize}
  \mpitem Increased storage capacity (more vectors can be summed and later retrieved)
  \mpitem Feedback matrix $\Amtx$ typically \emph{diagonal} since 2022 (see ``S4D'')\\
  	  \maybepause $\Rightarrow$ Parallel bank of vector one-poles (\emph{``linearly'' gated, state-expanded RNNs})
  \mpitem In Mamba-2, $\Amtx = p\,\Imtx$, \ie, \emph{shared memory duration} across expanded state
  % \mpitem Processed sequence (``context buffer'') is \emph{indefinitely long}
  % \mpitem Multiple parallel memories (as in ``multi-head attention'')
  \mpitem Gating matrices in Mamba[-2] are simple linear input projections:
%  \[
  $
          [\Bmtx(\xv_n), \Cmtx(\xv_n)] = \Lmtx\,\xv_n  % , \qquad \Dmtx(\xv_n) = \mbox{SiLU}(\Lmtx^\prime\xv_n)\]
  $
%  \]
  % (See Mamba, \eg)
  % \mpitem Conv1D mixing followed by SiLU on input for Mamba (only nonlinear activation)
  % \mpitem $\Cmtx$ could become an \emph{attention matrix} across the expanded state; $\Amtx$ could make it \emph{shift} like a
  %    \emph{transformer context buffer} (using a unit subdiagonal, \eg)
  % \mpitem $\Amtx = -p\Imtx$ as of Mamba-2, \ie, parallel RNNs all have the same memory duration
\end{itemize}

\end{slide}

\section[\sectopts,toc={Processing}]{Memory Access}

\begin{slide}[\slideopts,toc={Perceptrons}]{Detecting Multiple Vectors in Parallel}
\maybepause
\vspace{1em}
\textbf{Idea:} Detect multiple memory vectors in \emph{parallel} using an array of \emph{``Perceptrons''}
%\emph{``Multi-Layer Perceptron'' (MLP)}
\begin{itemize}
  \mpitem Each Perceptron detects one or more memory vectors similar to its weight vectors
  \[
  y_i(n) = \wv_i^T\hv(n) > b_i
  \]
  where $y_i(n)$ denotes the $i$th output at time $n$, i=0,\ldots,M-1,\\
  and $\hv(n)$ denotes the (``hidden'' [expanded-] state) vector memory
  \mpitem Note that the $\Cmtx$ matrix can provide these weights:
  \[
  \yv(n) = \Cmtx\,\hv(n) > \beev
  \]

\mpitem The Perceptrons indicate \emph{which weight-vectors $\wv_i$ are present} in the vector memory $\hv$

\mpitem \textbf{Idea:} (Backpropagation---1980s?): To facilitate
\emph{learning} $\wv_i$ via gradient descent, replace ``$>$'' by
something smoother, such as $1+\tanh[\Cmtx\,\hv(n) - \beev]$

\end{itemize}

\end{slide}

\begin{slide}[\slideopts,toc={Sequences}]{Sequence Modeling}
  \vspace{-1em}
  \begin{itemize}
    \mpitem If each vector represents a \emph{word,} a vector sum is simply a \emph{bag of words}
    \mpitem To model a \emph{sequence} of words, we have various \emph{sequence-position-encoding} options:
    \begin{enumerate}
      \mpitem \emph{Amplitude Decay} - Multiply the sum by a \emph{forgetting factor} each sequence step\\
      (RNNs) - \emph{poor choice} (conflates with angular distance on the hypersphere)
      \mpitem \emph{Sinusoidal Amplitude Modulation} - Add a sinusoid with \emph{increasing frequency} to each vector summing into the history\\
      (used in the original Transformer)
      \mpitem \emph{Phase Shift} - Multiply by the sum by $e^{j\Delta}$ each sample\\
      (``RoPE'') - \emph{apparently most used today}
    \end{enumerate}
    % Toward Xformer: \mpitem Use many vector-sum memories in parallel, positionally encoded (``State Expansion'' in SSMs)
    % \mpitem Learn position-specific \emph{FIR Coefficients} across a parallel bank of positionally encoded vector-sum memories (``Attention Layer'' in a Transformer)
  \end{itemize}
\end{slide}

\begin{slide}[\slideopts,toc={WRoPE}]{RoPE and WRoPE}

\vspace{-1em}

\begin{itemize}
\item Rotational Positional Encoding (RoPE) owns \emph{one arc direction} along the hypersphere % to \emph{positional encoding}
  \mpitem We can thus rotate our vector memory $\hv(n)$ by $\Delta$ radians each time step to ``age'' it:
  \[
  \hv_a(n) = e^{j\Delta}\hv(n), \quad \mbox{with}\; \Delta = \frac{2\pi}{L}
  \]
  when our maximum sequence length (before reset) is $L$
  \mpitem \textbf{Idea:} ``Warped RoPE'' (WRoPE) for \emph{arbitrarily long sequences}:
  \[
  \Delta_n = \frac{2\pi n}{n+L}, \quad n=0,1,2,\ldots
  \]
  \maybepause
  (inspired by the \emph{bilinear transform} used in digital filter design)
  \mpitem A \emph{blend} of \emph{uniform} and \emph{warped} rotations can be used:
  \[
  \Delta_n = \funcalign{\frac{\pi n}{L}}{n=0,1,2,\ldots,L-1}{%
    \pi + \frac{\pi n}{n+1}}{n=L,L+1,L+2,\ldots}
  \]
  where $L$ is now the \emph{typical} sequence length (giving it more
  ``space'' in recall)
\end{itemize}
\end{slide}

\begin{slide}[\slideopts,toc={}]{Searching a Time Range}

\vspace{-1em}

\begin{itemize}
  \mpitem Reserving complex angle for positional encoding $\Leftrightarrow$ \emph{real} vocabulary
          embedding vectors
  \mpitem Downstream weights and biases must be \emph{complex}
  \mpitem Complex angle discarded by absolute value when no longer needed
  \mpitem Exact orthogonality is obtained for each of the $L$ RoPE time-steps when there is 
  no amplitude decay as in normal RNNs, \ie, $e^{jn\Delta}\xv(n)\perp e^{jm\Delta}\xv(n)\, \forall n\ne m$
  \mpitem RoPE query vector for a vocabulary vector (``token'') $\wv$ any time between $n_1$ to $n_2$ is
   \[
   \qv \eqsp \sum_{n=n_1}^{n_2}e^{jn \Delta}\wv
   \eqsp \wv \frac{e^{j(n_2+1)\Delta} - e^{jn_1\Delta}}{e^{j\Delta}-1}
   \eqsp \wv\, e^{j\theta_{12}}\frac{\sin\left(\frac{n_2-n_1+1}{2}\Delta\right)}{\sin(\Delta/2)}
   \]
   \vspace{-1em}
\end{itemize}
\end{slide}

\begin{slide}[\slideopts,toc={Compressed Time}]{Compressed Time Retrieval}

\begin{itemize}
  \mpitem Recall that a vector $\wv$ is detected in the vector memory $\hv$ by the inner product $\wv^T\xv$
  exceeding some detection threshold $b$, \ie, $\wv^T\xv > b$
  \mpitem The threshold $b$ can be lowered to capture a wider range of similar vectors
  \mpitem The captured vectors lie in a cone at angle $\theta$ centered on the vector $\wv$,
  and $\cos(\theta)=b$
  \mpitem In WRoPE, the angular rotation decreases as $1/n$ or the like
  \mpitem ...
\end{itemize}

This causes inner products to increase as vectors get squeezed together along the complex-angle arc.

\end{slide}

\begin{slide}[\slideopts,toc={Reservations}]{Reserving Angular Dimensions}

  \vspace{-1em}

  Reserving the radial dimension looks easy (\eg, \tx{RMSNorm} every $k$ gradient-descent steps)\\
  How to reserve an \emph{arc dimension} for [W]RoPE and perhaps other purposes?

  \begin{itemize}
%    \mpitem \emph{Augment the space with new angular dimensions:}
%    \begin{enumerate}
      \mpitem RoPE: Effectively reserves \emph{complex angle:} $\hv_a(n) = e^{j\Delta}\hv(n)$\\
      (data size doubled: $\xv \to (\xv,\zv)$)
      \mpitem A zero imaginary part in the vocabulary embedding reserves complex angle % for [W]RoPE
      \mpitem \emph{Quaternions} give two more angles
      		$\Leftrightarrow$ embedding size quadrupled: $\xv \to (\xv,\zv,\zv,\zv)$\\
      \begin{itemize}
        \mpitem $\hv_a(n) = \qb\,\hv(n)$, where, from
        \htmladdnormallink{Wikipedia}{https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation}:\\
        $
        % \displaystyle
        \qb = e^{{\frac {\Delta}{2}}{(\av_x\ib +\av_y\jb +\av_z\kb )}}
        = \cos {\frac {\Delta }{2}}+(\av_x\ib +\av_y\jb + \av_z\kb )\sin {\frac {\Delta }{2}}
        = \cos {\frac {\Delta }{2}}+\av\sin {\frac {\Delta }{2}}
        $
      \end{itemize}
      \mpitem See the Cayley-Dickson Construction for $2^k-1$ angles, $k=1,2,3,\ldots$
      \mpitem Trivial in \emph{polar coordinates} (``write-protect'' reserved dimensions during gradient step),\\
      but then back-propagation must be rewritten for polar coordinates
      % \\ Replace weight matrices $\Wmtx$ with \emph{rotation matrices?}
      \mpitem Find a suitable method of \emph{constrained gradient descent}
      \mpitem Train (or calculate) a \emph{pointwise MLP} that ``de-rotates'' $\xv+\mu\gradv$ given also $\xv$
      \mpitem Use \emph{reserved translational dimension(s)} instead (``TraPE'') (omitted from \tx{RMSNorm})
      % Illustrate "meta" reserved translation dimension: \myFigureToWidth{threeGinaSpheres}{0.5\twidth}{}
  \end{itemize}

\end{slide}

\section[\sectopts,toc={Attention}]{Attention}

\begin{slide}[\slideopts,toc={Attention}]{Attention Layer}

\vspace{-1em}

\textbf{Idea:} Also use \emph{FIR Filtering} (SSM State Expansion Factor $N\ge M$, $\Amtx$ subdiagonal):

%\myFigureToWidth{one-pole-rnn-seq}{0.6\twidth}{} % {Sequence Modeling with Gated RNNs}
%\vspace{-4em}

\myFigureToWidth{attention}{0.8\twidth}{}

\vspace{-1em}

\emph{Separately learnable FIR coefficient matrices} $\dv_k[\xv(n)], \cv_j[\xv(n-j),k]$, depending on:
  \begin{enumerate}
    \mpitem input \emph{position} $j$ in the input sequence (``context buffer'' or ``expanded state'' + [W]RoPE)
    \mpitem input \emph{vector} $\xv(n-j)$,\; $j=0,1,2,\dots,M$
    \mpitem \emph{output-position} $k$ being computed,\; $k=0,1,2,\dots,M$ ($M+1$ outputs)
  \end{enumerate}

\maybepause
\textbf{Idea:} Add \emph{relevance gating} suppressing unimportant inputs to each output (``attention'')

\vspace{0.2em}

\maybepause
\textbf{Idea:} Create \emph{new embedding vectors} as \emph{sums} of relevant input vectors (``attention'')

\vspace{0.2em}

\maybepause
\textbf{Idea:} Measure relevance using an \emph{inner product} between the output and input positions (``dot-product attention'')
\end{slide}

\begin{slide}[\slideopts]{Dot-Product Attention}

\vspace{-2em}

\myFigureToWidth{attention}{0.8\twidth}{}

\vspace{-2em}

\textbf{Relevance Gating}

Let $\xv_k$ denote $\xv(n-k)$

The contribution from input $\xv_j$ to the nonlinear FIR sum for output $\yv_k$ can be calculated as
\\
$
\cv_{kj}\xv_j = \left[\left(\sum_{m\in \Rscr(\xv_k)}\xv_m\right)^T \xv_j\right]\xv_j
$
\\
\maybepause
or more generally $\zbox{\cv_{kj} = Q_k^T \xv_j}$, where
\\
%\begin{itemize}
%  \mpitem
$Q_k(\xv_k,k)$ is called the \emph{query} vector for position $k$ in the input sequence
%  \mpitem $K_j[\xv_j,j]$ is called the \emph{key} vector for position $j$ in the input sequence
%  \mpitem $V_j[\xv_j]$ is called the \emph{value} vector for position $j$ in the input sequence
%\end{itemize}

\vspace{1em}

\maybepause
The query $Q_k$ can be a sum of \emph{all vectors supported in the attention sum}:\\
$Q_k = \xv_k + \xv_{m_1} + \cdots + \xv_{m_k}$\\
$\Rightarrow (Q_k^T \xv_j)\xv_j \approx \xv_j$, if $\xv_j$ is similar to \emph{any vector} in the query sum.

%We still need a reason to generalize $\xv$ above to a \emph{key} $K(\xv)$ and \emph{value} $V(\xv)$

\end{slide}

%% \begin{slide}[\slideopts]{Queries, Keys, and Values}

%% \begin{itemize}
%%   A query $Q_k$ formed as a sum of sought vectors, looks useful, but we can generalize further
%%   \mpitem The query for a \emph{noun} can look for \emph{all verbs} and \emph{all adjectives} that could contextualize it
%%   \mpitem The model space can have different partitions for nouns, verbs, and adjectives
%%   \mpitem We can learn a projection $K(\xv)$ that projects any $\xv$ to a vector representing ``noun'', ``verb'', or ``adjective,'' etc.
%%   \mpitem Finally, instead of adding the original vector into the sum, we can learn a different \emph{value vector} $V(\xv)$
%%   for building attention-sums in some new sector of the model space, etc.
%% \end{itemize}

%% So now we're up to (Q,K,V) attention:
%% \[
%% \cv_{kj}\xv_j = (Q_k^T K_j)V_j
%% \]

%% %Similarly, the key vector $K_j$ can be a sum of \emph{all queries} to which it applies, \eg,\\
%% %$K_j = \xv_j + \xv_{n_1} + \cdots + \xv_{n_k} \Rightarrow (Q_k^T K_j)\xv_j \approx M\xv_j$, given $M$ similar vectors in both.


%% %% \textbf{Idea:} For more flexibility in the attention sum (or to learn
%% %% down-projections in \emph{multi-head} mode), replace $\xv_j$ in the
%% %% attention sum with a learned \emph{value vector} $V_j[\xv_j]$\\

%% \end{slide}

\begin{slide}[\slideopts]{Multi-Head Attention}

\textbf{Idea:} To support multiple meaning possibilities, \emph{partition the model space} into\\
parallel independent \emph{attention calculations} (``multi-head attention'')

%\myFigureToWidth{one-pole-rnn-seq}{0.6\twidth}{} % {Sequence Modeling with Gated RNNs}
%\vspace{-4em}

\begin{itemize}
  \mpitem Each \emph{attention head} can form an independent input interpretation
  \mpitem Useful for \emph{ambiguous} sequences, especially in the lower layers
  \mpitem Also introduced in the Transformer paper (2017)
\end{itemize}

\maybepause

Now we need \emph{down-projections} of the relevance-calculation components\\
$\Rightarrow$ relevance of input $j$ to output $k$ in attention-head $l$ becomes proportional to
\[
\cv_{kj}\xv_j = (Q_k^T \xv_j)\xv_j \;\longrightarrow\; \cv_{lkj}\xv_{lj} = \left[Q_{lk}^T(\xv_k) K_{lj}(\xv_{j})\right]V_{lj}(\xv_j)
\]
where $Q_{lk}$ (``query''), $K_{lj}$ (``key''), and $V_{lj}$
(``value'') vectors are learned \emph{down-projections} of the input $\xv_j$
for each attention-head $l$ and for all sequence indices $j$ and $k$
in the context buffer (``Transformer'')

%%   \mpitem Filter coefficients computed from input as before, but smaller

%% \[
%% $(Q_k^T K_j)V_j$,
%% \]
%% using down-projected queries $Q_k(\xv)$, \emph{keys} $K_j(\xv)$, and \emph{values} $V_j(\xv)$

\maybepause

Other useful generalizations can be imagined for these learned (Q,K,V)
vectors, such as grouping grammatical functions, creating new
model-space regions, etc.

\end{slide}

\begin{slide}[\slideopts,toc={Language}]{How Many Vectors in a Sum are Needed for Language?}

\maybepause
\vspace{-1em}

It is well known that \emph{phone numbers} were limited to \emph{7
digits} due to \emph{short-term memory limits} in typical
humans. \maybepause Can \emph{language} be parsed using 7 vectors
or less at each level?

\maybepause

\textbf{Layers:}
\vspace{-1em}
\begin{multicols}{2}
\begin{enumerate}
 \mpitem Base vocabulary = characters\\
  (26 for English)
  \mpitem Syllable in 7 chars or less\\
  (44 syllables in English;\\
   107 in Int'l Phonetic Alphabet)
  \mpitem Word in 7 syllables or less
  \mpitem Noun + 6 or less modifying adjectives 
  \mpitem Verb + up to 6 adverbs
  \mpitem Noun phrase
  \columnbreak
  \mpitem Direct or indirect object
  \mpitem Prepositional phrase
  \mpitem Subject verb [indirect object] object
  \mpitem Sentence
  \mpitem Paragraph
  \mpitem Section
  \mpitem Chapter
  \mpitem Book
 \mpitem Subject Area % $\ldots$
%  \mpitem $\ldots$
\end{enumerate}
\end{multicols}
\maybepause
\vspace{-1em}
Different cortical areas (6 layers each) needed for all these levels.\\
\maybepause
\textbf{Complex examples:}
{\tiny
\url{https://www.quora.com/In-regards-to-diagramming-sentences-which-one-is-the-most-difficult-youve-ever-come-across}
}
\end{slide}

% Relatively weak/obvious points:
%% \begin{slide}[\slideopts]{Weight Tying}

%%   When the sequence model maps input to output in the same ``language'' (e.g., English to English),
%%   it makes sense to use the \emph{same embedding vectors} at the input and output layers, instead
%%   of separately learning a set of weights for mapping to the final output.
%%   This is called ``weight tying'' (many fewer parameters, better results).

%% \end{slide}

%% \begin{slide}[\slideopts]{Hierarchical Blocks}
%% \begin{itemize}
%%   \mpitem Cascade blocks of attention + MLP and/or gated recurrence + MLP to model \emph{hierarchical relationships} like image features or grammatical constructs
%%   \mpitem Attention and gated RNNs are called ``mixing layers'' (successive inputs are combined)
%%   \mpitem MLPs are called ``point transformations'' (general mapping of any vector from one place to another)
%%   \mpitem RMSNorm typical at the input to put it on the hypersphere --- also used internally\\
%%   (see Hawk/Griffin e.g.)
%%   \mpitem Thus, these ``point MLPs'' are effectively data-dependent \emph{rotation matrices}.\\
%%   In model-dimension $N$, we need only learn $N-1$ \emph{rotation angles} for each input class.\\
%%   The direct mapping of the coordinates used now seems efficient enough, but projection back to the sphere
%%   during training might help.  Then perhaps there is no need for RMSNorm at the output after summing the skip
%%   connection.
%% \end{itemize}
%% \end{slide}

%% \begin{slide}[\slideopts,toc={Other Features}]{Other ``Obvious'' Features}
%% \begin{itemize}
%%   \mpitem Tying output ``language modeling head'' weights to the input embedding weights
%%   \mpitem Positional encoding within an RNN
%% \end{itemize}
%% Less obvious:
%% \begin{itemize}
%%   \mpitem Multihead Attention (down-projection + independent spatial processing)
%%   \mpitem (Q,K,V) matrices for ``dot-product attention'' (down-projection + inner-product function + value flexibility)
%% \end{itemize}
%% \end{slide}

%% \begin{wideslidewhite}[\slideopts,toc={Architectures}]{Architectures}
%% \vspace{-4em}
%% \myFigureToWidth{Architectures}{\twidth}{}
%% \vspace{-4em}
%% \end{wideslidewhite}

\begin{slide}[\slideopts,toc={Unified State Space}]{State Space Unification of Transformers and GRNNs}
\vspace{-1em}
% State Transition Matrices
\begin{equation*}
\begin{array}{cc}
\underbrace{
\mathbf{A_T} = \ev^{j\Delta_n} \begin{pmatrix}
0 & 0 & \cdots & 0 & 0 \\
\onevec & 0 & \cdots & 0 & 0 \\
0 & \onevec & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \cdots & \onevec & 0
\end{pmatrix}
}_{\mbox{Transformer}}
&\quad
\underbrace{
\mathbf{A_M} = \av_n \ev^{j\Delta_n} \begin{pmatrix}
\onevec & 0 & \cdots & 0 \\
0 & \onevec & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \onevec
\end{pmatrix}
}_{\mbox{Mamba-2 style RNN + [W]RoPE}}
\end{array}
\end{equation*}
\[
\mathbf{A_{TM}} = \ev^{j\Delta_n} \begin{pmatrix}
0 & 0 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
\onevec & 0 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
0 & \onevec & \cdots & 0 & 0 & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \onevec & 0 & 0 & \cdots & 0 \\
% & & & & & \av_n \onevec & 0 & \cdots & 0 \\
0 & 0 & \cdots & 0 & \bv_{1}(n) & \av_n & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 0 & \bv_{N_M}(n) & 0 & \cdots & \av_n
\end{pmatrix}
\quad
\mathbf{B_{TM}} = \begin{pmatrix} \beev_1(n) \\ 0 \\ 0 \\ \vdots \\ 0 \end{pmatrix}
\]

\end{slide}

\begin{slide}[\slideopts,toc={TransMamba}]{Transformer followed by GRNN with 2x State Expansion (like Mamba)}
\vspace{-1.6em}
\myFigureToWidth{transmamba}{\twidth}{TransMamba} % MambaFormer is taken
\vspace{-2em}
\maybepause
\myFigureToWidth{cartoon-mamba-t}{0.9in}{} % And now we don't want MambaFormer after all - this is great!
% Prompt 1: "Friendly cartoon Mamba snake" (cute but boring)
% Prompt 2: "Make it trans" (over the top better!)
\end{slide}

\begin{slide}[\slideopts,toc={Plan}]{Next Steps}
  \vspace{-1em}
  \begin{itemize}
    \mpitem Try to improve [Trans]\textbf{Mamba}[-2] on small synthetic datasets testing \emph{memory}
    \begin{itemize}
      \mpitem Vocabulary embeddings trained to the unit hypersphere (\eg, \tx{word2sphere})
      \mpitem Memory \emph{duration} and \emph{reset} functions separately trained and implemented
      \mpitem Initial \emph{biases} at $\zv$ versus $L/N$ etc.
      \mpitem Do \emph{power normalization} in place of \tx{RMSNorm} where possible (efficiency)
      \mpitem Try \emph{power normalized attention} in place of $1/\sqrt{d_h}$ and \tx{Softmax} (efficiency)
      \mpitem Adapt \emph{model dimension} to \emph{layer width} at each level (efficiency)
      \mpitem Warped Rotational Positional Encoding (WRoPE)
      \mpitem Translational Positional Encoding (TraPE) in its own head (no \tx{RMSNorm})
      \mpitem Explore other ``Control Heads'' that flow along purely for ``conditioning'' like TraPE
    \end{itemize}
    \mpitem Progress to date:
    \begin{itemize}
      \mpitem New synthetic benchmarks analogous to ``needle in a haystack''
      \mpitem Adapted Andrej Karpathy's \tx{makemore} code, adding Mamba and new benchmarks
      \mpitem Four papers started, aiming for Arxiv, GitHub, ``AI social media,'' blog
    \end{itemize}
    \mpitem Feel free to take over any of these! (and LMK so I can do something else)
  \end{itemize}
\end{slide}

\begin{slide}[\slideopts,toc={Hypersphere}]{Thanks for your Attention!}
\vspace{-1em}
\myFigureToWidth{ginaSphere1}{\twidth}{}
\end{slide}

\section[\sectopts,toc={History Samples}]{Sequence Modeling Snapshots}

\begin{slidewhite}[\slideopts, toc={LSTM \& GRU}]{LSTM and GRU}
\vspace{-6em}
\myFigureRotateToWidth{Architectures1}{-90}{\twidth}{}
\end{slidewhite}

\begin{slidewhite}[\slideopts, toc={SSM \& Mamba}]{Structured State Space and Mamba}
\vspace{-6em}
\myFigureRotateToWidth{Architectures2}{-90}{\twidth}{}
\end{slidewhite}

\begin{slidewhite}[\slideopts,toc={Hawk \& Griffin}]{Hawk and Griffin}
\vspace{-6em}
\myFigureRotateToWidth{Architectures3}{-90}{\twidth}{}
\end{slidewhite}

\begin{slidewhite}[\slideopts,toc={HGRN2}]{Gated ``Linear'' RNNs with State Expansion}
\vspace{-6em}
\myFigureRotateToWidth{Architectures4}{-90}{\twidth}{}
\end{slidewhite}

\begin{slidewhite}[\slideopts,toc={RWKV+}]{RWKV, Eagle, Finch}
\vspace{-6em}
\myFigureRotateToWidth{Architectures5}{-90}{\twidth}{}
\end{slidewhite}

%\begin{slidewhite}[\slideopts,toc={}]{Architectures 6}
%\vspace{-6em}
%\myFigureRotateToWidth{Architectures6}{-90}{\twidth}{}
%\end{slidewhite}

%\input ai-reading-2022.tex
%\input ai-reading-2023.tex
%\input ai-projects.tex
%\section[\sectopts]{Differentiable DSP (DDSP)}
%\input ddsp.tex

\end{document}
\endinput
